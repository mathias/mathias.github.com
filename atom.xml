<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[blog.mattgauger.com]]></title>
  <link href="http://blog.mattgauger.com/atom.xml" rel="self"/>
  <link href="http://blog.mattgauger.com/"/>
  <updated>2014-03-13T19:28:27-05:00</updated>
  <id>http://blog.mattgauger.com/</id>
  <author>
    <name><![CDATA[Matt Gauger]]></name>
    <email><![CDATA[contact@mattgauger.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A theory of compound intelligence gain]]></title>
    <link href="http://blog.mattgauger.com/blog/2014/01/12/a-theory-of-compound-intelligence-gain/"/>
    <updated>2014-01-12T14:02:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2014/01/12/a-theory-of-compound-intelligence-gain</id>
    <content type="html"><![CDATA[<p>Note that this is probably not enough to call a theory. It&#8217;s an idea, at most.</p>

<p>I&#8217;m currently reading the book <a href="http://www.amazon.com/gp/product/0984725113">Race Against the Machine</a>, which describes how increasing levels of automation by technology are related to capital and labor. But this post isn&#8217;t about that book. It simply triggered me to think about my motivations for my current side projects, and how I might explain to others why exactly I think that my current side projects are so important.</p>

<p>While <em>Race Against the Machine</em> describes technological progress as a force that leaves behind skilled workers who no longer have relevant skills, my thinking is on intelligence augmentation, and how I can use my own knowledge and programming skills to build tools that increase my own effectiveness and ability to perform my job. Namely, how can I write software that improves my cognition and memory such that I am better at writing software, and gain other benefits from having increased cognition and memory?</p>

<p>Douglas Engelbart <a href="http://www.dougengelbart.org/pubs/augment-3906.html">wrote extensively</a> about augmenting intelligence, primarily with improving workflows and then with computer software. I&#8217;ve previously <a href="http://blog.mattgauger.com/blog/2013/03/17/by-augmenting-human-intellect/">quoted him</a> on this blog. I feel that part of that quote bears repeating here:</p>

<blockquote>
By &#8220;augmenting human intellect&#8221; we mean increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems.
<footer>
<strong>Douglas C. Engelbart</strong>
&ndash;
<cite><a href="http://www.dougengelbart.org/pubs/augment-3906.html">Augmenting Human Intellect: A Conceptual Framework </a></cite>
</blockquote>


<p>Of course, Engelbart was writing about this in 1962 &#8211; well before every home had a personal computer and everyone had a powerful supercomputer in their pocket. For a modern overview of Engelbart&#8217;s framework, see <a href="http://fluid.media.mit.edu/sites/default/files/The%20Design%20of%20Artifacts%20for%20Augmenting%20Intellect.pdf">The Design of Artifacts for Augmenting Intellect</a>.</p>

<p>My earliest encounters with concepts of intelligence augmentation most likely come from science fiction. One character that has inspired a lot of my work (and that I&#8217;ve probably told you a lot about if we&#8217;ve discussed this project in person) is Manfred Macx from Charles Stross&#8217;s <a href="http://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html">Accelerando</a>. Macx is described in the early parts of the book as having a wearable computer that acts as his <em>exocortex</em>. The idea of an exocortex being that some part of his memory, thinking, and information processing lives outside of his head and on the wearable computer. Similarly, the exocortex can help act as a gate to his attention, which is one of our limited resources.</p>

<p>If you think about it, just as <a href="http://www.ted.com/talks/amber_case_we_are_all_cyborgs_now.html">we are all cyborgs now</a> by virtue of the technology we use every day, we are also all on our way to having exocortexes. Many of us use Gmail filters to protect our attention spans from email we receive but don&#8217;t always need to read. Or we use Google search to add on to our existing memory, perhaps to remember some long-forgotten fact that we only have an inkling of.</p>

<p>I&#8217;ve had Manfred Macx&#8217;s exocortex (and other flavors of science fiction&#8217;s wearable computers and augmented intelligences) kicking around in my head for years. Gmail tells me that I was trying to plan the architecture for such a thing as far back as 2006. It&#8217;s taken a lot of thinking and further learning in my career to even get to the point where I felt ready to tackle such a project.</p>

<p>What I am setting out to build is an exocortex of my own design, under my own control. Not something that is handed to me by Google in bits and pieces. And to do so, it turns out, requires a lot of research and learning. There&#8217;s tons of research on the topics of proactive autonomous agents, text classification, and wearable computing that I have been reading up on. Just to build the first phase of my project, I have been learning all of the following:</p>

<ul>
<li><a href="https://github.com/clojure/core.logic">core.logic</a> (which is based on Prolog, so I&#8217;m learning some Prolog now, too)</li>
<li><a href="https://github.com/clojure/core.async">core.async</a> (Clojure&#8217;s implementation of C.A.R. Hoare&#8217;s <a href="http://www.amazon.com/Communicating-Sequential-Processes-International-Computing/dp/0131532715/">Communicating Sequential Processes</a>, which is also how Go&#8217;s goroutines work)</li>
<li><a href="http://cascalog.org/">Cascalog</a> and <a href="http://hadoop.apache.org/">Hadoop</a>, to do my distributed computing tasks</li>
<li><a href="http://www.datomic.com/">Datomic</a> &amp; Datalog (a subset of Prolog for querying Datomic), to store knowledge in a historical fashion that makes sense for a persistent, lifelong knowledge system</li>
<li>Topic clustering, text classification, and other natural language processing approaches</li>
<li>Data mining, and in particular, streaming data mining of large datasets on Hadoop clusters, by reading the Stanford textbook <a href="http://infolab.stanford.edu/~ullman/mmds.html">Mining of Massive Datasets</a></li>
<li>Generally learning Clojure and ClojureScript better</li>
<li>and probably more that I am forgetting to mention</li>
</ul>


<p>Of course, if I look at that list, I can be fairly certain that this project is already paying off. These are all things that I had very little experience with before, and very little reason to dig into so deeply. Not represented here are the 40 or so academic papers that I identified as important, and seriously set out to read and take notes on &#8211; again, probably learning more deeply these topics than I otherwise would have.</p>

<p>Which brings me to this theory, the idea of this post: That by even beginning to work on this problem, I&#8217;m seeing some gains, and that any tools I can build that give me further gains will only compound the impact and effectiveness. <strong>Improving cognition and learning compounds to allow further gains in cognition and learning.</strong></p>

<p>There&#8217;s some idea in the artificial intelligence community that we don&#8217;t need the first general artificial intelligence to be built as a super-intelligence; we need only build an artificial intelligence that is capable of improving itself (or a new generation of artificial intelligence.) As each generation improves, such intelligences could become unfathomably intelligent. But all it takes is that first seed AI that can improve the next.</p>

<p>So for improving our own human intelligences, we may not need to build a single device up-front that makes us massively intelligent. We only need take measures to improve our current knowledge and cognition, to build tools that will help us improve further, and continue down this path. It will definitely not be the exponential gains predicted for AI, and may not be even linear &#8211; that is, the gains in cognition from building further tools and learning more may plateau. But there will be improvements.</p>

<p>For that reason, I&#8217;m not setting out to build Manfred Macx&#8217;s exocortex from the beginning. Instead, I have been building what I describe as a &#8220;Instapaper clone for doing research&#8221; &#8211; a tool that, if it improves my existing ability to research and learn new topics, could pay off in helping me to build the next phase of my projects.</p>

<p>Of course, at the same time, I have an eye towards using the foundation of this tool as the datastore and relevance-finding tool for the overall project. Such a tool can automatically go and find related content &#8211; either things I have read, or simply crawl related content on the web. Eventually, this tool will also ingest all of the information I interact with on a daily basis: every website I browse, every email I receive, every book that I read. A searchable, tagged, annotatable reference with full metadata for each document as an external long-term memory. But this is all a topic for another post.</p>

<p>This, in concert with what current research tells us is effective: <a href="http://www.salon.com/2013/12/29/sciences_obsession_the_search_for_a_smart_pill/">improved nutrition and supplementation, exercise, meditation, and N-back training</a>, may just be my ticket to higher levels of human intelligence. But for now, I just want the early-adopter edge. I want to see how far I can push myself on my own skills. Some large corporation may be able to field hundreds of developers to create a consumer product for the public that benefits everyone in similar ways &#8211; but I might be able to do this for myself years ahead of that. And wouldn&#8217;t that be cool?</p>

<p>And this is where I call it a theory: it could very well be that there&#8217;s no such thing as compounding interest on intelligence. Only time and my own experiences with this project will tell me.</p>

<p>If you&#8217;ve made it this far and you&#8217;re interested in this kind of stuff, that is: intelligence augmentation, wearable computing, autonomous proactive agents, etc., <a href="https://twitter.com/mathiasx">get in touch</a>. There doesn&#8217;t seem to be much of an online community around these topics, and I&#8217;d like to start creating one for discussion and organizing open source projects around these topics.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An (unscientific) study in behavior change with software]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/12/28/an-unscientific-study-in-behavior-change-with-software/"/>
    <updated>2013-12-28T08:54:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/12/28/an-unscientific-study-in-behavior-change-with-software</id>
    <content type="html"><![CDATA[<p>Forming habits is hard. There&#8217;s been tons of research on what practices help form new habits successfully. And there has been research on what software can do to help form new habits. It&#8217;s not enough to simply send daily reminders or keep track of the goals in a visible place. For software to help us form new habits successfully, we must look to the current research for clues as to how habits are formed.</p>

<p>Over the past year or so, I&#8217;ve been trying to adopt a habit to take Vitamin D every morning. I&#8217;ve been largely successful, which I think is partly due to the software I used. I use <a href="http://lift.do">Lift</a> on my iPhone, which sends me emails every morning as a reminder. The app itself has checkins for each habit, progress charts, and social features. Most mornings, I wake up, swipe away the reminder email, and take my morning antihistamine and a Vitamin D. <a href="#behavior-footnote-1" name="behavior-footnote-1-return">[1]</a> Like I said, I&#8217;ve been mostly successful, and at this point, I&#8217;ve taken Vitamin D every day in a row for 442 days in a row. <a href="#behavior-footnote-2" name="behavior-footnote-2-return">[2]</a>  Granted, taking a vitamin every morning is only a small change, but it is one that I wanted to accomplish and did. Small successes add up to bigger successes, and this gives me confidence that if I set out to make a bigger change in my life, I have a toolset that will help me to accomplish that goal.</p>

<p>So what does the research say helps us form successful habits? The Fogg Method <a href="#behavior-footnote-3" name="behavior-footnote-3-return">[3]</a> is one of the more well-known systems, and suggests that a way to be successful is to:</p>

<ol>
<li>Select the right target behavior.</li>
<li>Make the target behavior easy to do.</li>
<li>Ensure a trigger will prompt the behavior.</li>
</ol>


<p>So what do each of these steps tell us?</p>

<h2>The Right Target Behavior</h2>

<p>It&#8217;s hard to be successful in picking up a habit that you don&#8217;t already want to accomplish. Some things you may already want to do include things like learning a language, eating a specific diet, or flossing your teeth. It goes without saying that things you&#8217;d rather not do are going to be harder to implement.</p>

<p>But there&#8217;s another factor in play here that I think determines the <strong>right</strong> target behavior: simplicity. That is, is the habit a simple task to accomplish, or is it something complex and unmanageable? Can you perform one simple task per day and call it &#8220;done&#8221;, or is it more complicated as to whether it is &#8220;done&#8221; or not each day? The simple &#8220;done&#8221; state seems really important, and so it is good to focus on using this technique for binary actions: either you did them today, or you didn&#8217;t. Things that must be done with complicated schedules, every other day, or once a week, will be much harder to establish as habits.</p>

<h2>Easy to do</h2>

<p>One reason we want a simple target behavior is so that it is easy for us to add to our schedule. You may have a goal of exercising more. But &#8220;exercising more&#8221; doesn&#8217;t have a binary action associated with it; for example: what is &#8220;more&#8221;? Instead, you might say, &#8220;I want to exercise 45 minutes per day.&#8221; And that would be a much better goal. But if exercising means you have to drive to the gym, and the gym is out of your way each day, it might be very unlikely that you will do it. This is not a simple target behavior.</p>

<p>If you do have some goal that may not be simple to implement at first &mdash; say, the example of having to drive out of your way to the gym &mdash; instead try to find a simpler version of the habit that you can adopt first. You may decide instead to just do some bodyweight exercises before you leave for work each morning. Decide on the exercises and write them down. Either you did them or you didn&#8217;t. Later on, you can modify this existing habit to be more exercise, but for now, focus on what you can reasonably adopt as a simple habit.</p>

<p>The other concern in implementing an &#8220;easy&#8221; habit is how much time the new habit will take. In the above example, the initial goal was something like 45 minutes per day. Eventually, you could probably find when you exercise best and are least likely to schedule appointments (say, early morning or late at night), and actually implement that goal. But early on, it&#8217;s going to be hard to change your schedule for your new habit. I ran into this frequently while trying to find time after 5PM but before dinner to practice guitar. It didn&#8217;t help that after-work and dinnertime are frequently scheduled as social events, and that I have a habit of staying at the office past 5; all these added up to very little success in trying to spend 45 minutes to an hour practicing guitar at home after work.</p>

<h2>Triggers</h2>

<p>The last step is quite important. Where you might think of triggers as things like alerts on your phone or daily emails from a service like <a href="http://lift.do">Lift</a>, I didn&#8217;t find those kinds of prompts very effective in helping me adopt a habit.</p>

<p>To be more likely to perform some task on any given day, look at the habits you already have. I&#8217;ve been taking an antihistamine every morning since I was about 12; this has been a constant in my life and part of my routine for a very long time. Since I already have this daily habit, I added taking Vitamin D every morning to that habit. Other habits with no daily routine to hinge off of, like practicing guitar, were much harder to make stick.</p>

<p>Flossing is an easy addition to brushing your teeth every night, and just took enough of me making it simpler (finding a brand of flossers I liked rather than wrangling loose floss) and doing it enough times before it stuck, too.</p>

<h2>What didn&#8217;t work for me?</h2>

<p>As noted above, despite a couple attempts to really make daily guitar practice stick, I&#8217;ve never been able to tackle that habit. There were no good triggers that I could add the event on to, and I frequently didn&#8217;t have time for what I was trying to accomplish. If I were to go back to trying to focus on guitar, I&#8217;d probably start with much less time commitment, and schedule it some time when I&#8217;m very likely to be home and have 5-10 minutes, like early in the morning before work. Whether or not guitar practice is effective with my first cup of coffee would have to be tested, of course.</p>

<p>What I&#8217;ve found is that I&#8217;m partially motivated by progress bars and graphs, though, and so I will make time in my day for easy-to-accomplish things. So when I can, I will try to squeeze in some mundane activity I&#8217;m tracking in Lift, like washing the dishes. <a href="#behavior-footnote-4" name="behavior-footnote-4-return">[4]</a></p>

<p>The social component of Lift, on the other hand, doesn&#8217;t really help me any. For others, it might be a good motivator. In cycling, I have several local friends, including one local cyclist who is quite prolific and who frequently rides 10x as much as I do in a given week. We all use <a href="http://www.strava.com/">Strava</a> to track our cycling, and the social component alerts me to new rides that the prolific cyclist has done. Seeing that cyclist&#8217;s rides helps remind me to get out and enjoy more cycling, as well as sets up a nice carrot-on-a-stick for me to ride more to &#8220;catch up.&#8221; In that case, the social features definitely help me to perform an action more, but I wouldn&#8217;t really call cycling a habit as much as my transportation and leisure-time hobby that I can do whenever I have time.</p>

<p>Habits with no simple binary action and no triggers, such as creative acts, are especially hard to form as habits. I have tracked writing blog posts in Lift for some time, but since I only write blog posts when the mood strikes me, it is hardly a daily goal, and it would be difficult for me to implement the above steps to form an actual habit of blogging on a daily basis.</p>

<h2>Final thoughts</h2>

<p>Notice that most of the guidelines above have very little to do with software? Software itself can&#8217;t convince you to go to the gym or make you more likely to floss. But it can provide some prompts and some encouragement, and that might be enough to get you over the hurdle of adopting a new habit.</p>

<p>As with anything, you are an individual and your mileage may vary. Experiment, use an app like Lift or something else you prefer to track your progress, and see where it takes you.</p>

<p>There&#8217;s more resources out there to help understand forming new habits, self-control, and behavior change, but I feel like this is the baseline one needs to know to be more successful in implementing behavior change. Some references of note that I have been consuming:</p>

<ol>
<li><a href="http://shop.oreilly.com/product/0636920030201.do">Designing for Behavior Change</a>. Stephen Wendel. 2013.</li>
<li><a href="http://www.nytimes.com/2011/09/04/books/review/willpower-by-roy-f-baumeister-and-john-tierney-book-review.html?pagewanted=a0l&amp;_r=0">The Sugary Secret of Self-Control</a>. New York Times. September 2, 2011.</li>
<li><a href="http://pragprog.com/book/jkthp/the-healthy-programmer">The Healthy Programmer</a>. Joe Kutner. 2013.</li>
</ol>


<p>If you&#8217;re interested in some research, the above book (Designing for Behavior Change) is a good reference, as well as these papers:</p>

<ol>
<li><a href="http://dspace.mit.edu/handle/1721.1/79306">ReflectOns : mental prostheses for self-reflection</a> (hardware and software solutions)</li>
<li><a href="http://captology.stanford.edu/wp-content/uploads/2010/10/Fogg-and-Hreha-BehaviorWizard.pdf">Behavior Wizard: A Method for Matching Target Behaviors with Solutions</a></li>
</ol>


<hr />

<p><a name="behavior-footnote-1"></a></p>

<p><strong>1</strong> Yes, I&#8217;m aware of the fact that taking vitamins with an antihistamine decreases the effectiveness of the antihistamine. I&#8217;ll cover why I take them at the same time in this post. <a href="#behavior-footnote-1-return">&#8617;</a></p>

<p><a name="behavior-footnote-2"></a></p>

<p><strong>2</strong> You can view my progress on Lift <a href="https://lift.do/users/34b3bcceda0808f3c096">on my public profile</a>. Notice there&#8217;s quite a few habits I&#8217;ve tried to form with Lift in the past that didn&#8217;t quite work. <a href="#behavior-footnote-2-return">&#8617;</a></p>

<p><a name="behavior-footnote-3"></a></p>

<p><strong>3</strong> As described by BJ Fogg in the preface to <a href="http://shop.oreilly.com/product/0636920030201.do">Designing for Behavior Change</a>. <a href="#behavior-footnote-3-return">&#8617;</a></p>

<p><a name="behavior-footnote-4"></a></p>

<p><strong>4</strong> We don&#8217;t have a dishwasher in our current apartment, and I both dislike dirty dishes and dislike washing dishes by hand. <a href="#behavior-footnote-4-return">&#8617;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[In the year 2100]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/12/16/in-the-year-2100/"/>
    <updated>2013-12-16T23:41:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/12/16/in-the-year-2100</id>
    <content type="html"><![CDATA[<p>Recently I was asked by a coworker to write up some ideas for where our company would be in the future. Not just next year, or in 5 years, but where we saw the company in the year 2100. For my other coworkers, the year 2100 probably represents far enough out to ensure they stop thinking in the constraints of right now. But for someone who has read as much science fiction as I have, I saw a wide gulf of time.</p>

<p>So I hit up Wolfram Alpha, asking myself what technology might look like in the year 2100.</p>

<p>If Moore&#8217;s Law holds, then processors could contain 3.5 x 10<sup>22</sup> transistors &mdash; roughly 350 times more transistors than the number of grains of sands on the planet. (Thanks, Wolfram Alpha!) This also represents roughly 2.9x10<sup>18</sup> MIPS per chip, which is roughly a factor of ten more processing power in each chip than all 7 billion brains of humans on the planet, combined. That kind of computing power is almost unimaginable to me now.</p>

<p>While I don&#8217;t have a religious belief in the Singularity, I <strong>do</strong> think that we can&#8217;t really predict what it&#8217;ll mean to have so much computing power available to us. Or what technology, society, or people will look like by then. Of course, someone&#8217;s gotta write the software to make that hardware usefulâ€¦</p>

<hr />

<p><small>
Note: It could be that I messed up these numbers a bit; I went off the current transistor count and MIPS for the Intel i7-4770k processor, since I recently started putting together a server with one. And the numbers are extrapolated out quite a bit. If you&#8217;ve got corrections to these numbers, hit me up on <a href="https://twitter.com/mathiasx">Twitter</a> to let me know!
</small></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apprentice Talk Video &amp; Notes]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/11/11/apprentice-talk-video-and-notes/"/>
    <updated>2013-11-11T19:22:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/11/11/apprentice-talk-video-and-notes</id>
    <content type="html"><![CDATA[<p>Back in September, I had a blast speaking at <a href="http://nickelcityruby.com/">Nickel City Ruby</a>. My talk was entitled &#8220;Apprenticeship: Software Craftsmanship&#8217;s Missing Link&#8221; and included a lot of slides of sasquatches. The video has been posted to Confreaks, so I&#8217;m embedding it here. Also note that I have posted my notes and a few resources over on the microsite I created for this talk, <a href="http://blog.mattgauger.com/apprenticeship/">http://blog.mattgauger.com/apprenticeship/</a></p>

<iframe width="640" height="360" src="http://blog.mattgauger.com//www.youtube.com/embed/zuL7rAwmwCY" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A simple text editor foot pedal]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/08/06/a-simple-text-editor-foot-pedal/"/>
    <updated>2013-08-06T15:06:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/08/06/a-simple-text-editor-foot-pedal</id>
    <content type="html"><![CDATA[<p>When I first starting talking about <a href="http://blog.mattgauger.com/blog/2013/08/03/building-a-chording-keyboard-lessons-learned-and-progress-so-far/">building a chording keyboard</a>, both in person and online, people began to ask me about the possibility of building a computer foot pedal. With the Teensy microcontroller, a foot pedal would only need to watch a single digital input and output a few characters to the OS; much simpler than my chording keyboard project.</p>

<h2>Commercial products</h2>

<p>There are quite a few commercial products you can buy. They come with their own caveats:</p>

<ul>
<li><a href="http://www.kinesis-ergo.com/fs-savant-elite.htm">Kinesis</a> makes several, but they <a href="http://www.kinesis-ergo.com/fs-elite-single-compatibility.htm">can only be programmed on Windows</a>. They are also very expensive.</li>
<li>The <a href="http://www.xkeys.com/xkeys/xkfoot.php">Xkeys Foot Pedals</a> look good, but they are also expensive.</li>
<li>The DealExtreme <a href="http://dx.com/p/usb-triple-action-foot-switch-keyboard-control-foot-pedal-56508">Triple Action Foot Pedal</a> provides three buttons and is relatively cheap; however, the reviews I&#8217;ve read say the reliably is poor. This product can only be programmed on Windows.</li>
</ul>


<h2>Building the foot pedal</h2>

<p>My first idea for a foot pedal was to use DIY guitar pedal hardware. Such guitar pedals have nice, sturdy aluminum project boxes, robust foot switches, and look pretty good. But the ergonomics of such a pedal worried me: while a guitarist momentarily stomps on guitar pedals to turn them on or off while standing, a programmer would likely be tapping and/or holding a foot pedal for long periods of time, most likely from a seated position.</p>

<p><img src="http://blog.mattgauger.com/images/guitar_pedal_true_bypass_looper.JPG" alt="true bypass looper" style="display: block; margin: 0 auto;" /></p>

<p>Such a pedal would look something like the above, but with a USB cable coming out of it.  If you wish to use guitar pedal hardware, I suggest checking out <a href="http://www.mammothelectronics.com/">Mammoth Electronics</a>. (I receive no compensation for mentioning them; I have been a happy customer for several years.) For a two-to-three switch foot pedal, I suggest the 1590BB enclosures, and Mammoth can drill them in several ways for you.</p>

<p>My coworker <a href="http://sencjw.com/">Chris Wilson</a> suggested that I try a digital piano foot switch. These are relatively cheap and extremely sturdy. I picked up an <a href="http://www.amazon.com/M-Audio-Sustain-Pedal-Action-Keyboards/dp/B00063678K/">M-Audio Sustain Pedal</a> from Amazon for about $17.</p>

<p>The sustain pedal is designed to plug into a digital piano, so it has a 1/4&#8221; audio plug on its cable. In the past, I&#8217;ve built a lot of guitar effects pedals and some amps, and so I have a lot of 1/4&#8221; jacks around.</p>

<p>The wiring is simple: The Teensy (and most Arduinos) can do input pullup resistors for us. I mocked up the circuit with a little breadboard (ignore the weird angle of the Teensy here; it was required to get the pins into this small breadboard.) I downloaded some example button code to the Teensy and verified that it was working.</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9454682228/" title="IMG_1897 by Matt Gauger, on Flickr"><img src="http://farm4.staticflickr.com/3725/9454682228_4bb073ea47_z.jpg" width="480" height="640" alt="IMG_1897" style="display: block; margin: 0 auto;" /></a></p>

<p>To assemble, we simply solder a digital IO pin to one side of the 1/4&#8221; jack, and the other wire gets soldered to ground pin. My button is going to be connected on pin 9.</p>

<p>I had a small project box, much bigger than the Teensy really needed, but suitable for the job. I used my Dremel to cut a round hole for the 1/4&#8221; jack, and a rectangular slightly bigger than the mini-USB cable plug for our USB cable.</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9451903305/" title="IMG_1898 by Matt Gauger, on Flickr"><img src="http://farm4.staticflickr.com/3789/9451903305_4c6a3e0c35_z.jpg" width="480" height="640" alt="IMG_1898" style="display: block; margin: 0 auto;" /></a></p>

<p>Test fitting the Teensy in the project box:</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9451906889/" title="IMG_1900 by Matt Gauger, on Flickr"><img src="http://farm4.staticflickr.com/3718/9451906889_8e5624fa7d_z.jpg" width="640" height="480" alt="IMG_1900" style="display: block; margin: 0 auto;" /></a></p>

<p>Lastly, I used some velcro inside to attach the Teensy to the project box. All done with assembly!</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9451907469/" title="IMG_1902 by Matt Gauger, on Flickr"><img src="http://farm8.staticflickr.com/7451/9451907469_e6e6fc661f_z.jpg" width="480" height="640" alt="IMG_1902" style="display: block; margin: 0 auto;" /></a></p>

<p>The next step is to program the Teensy to send the key events we want. In this case, my coworker <a href="https://twitter.com/inyourdom">Josh</a> suggested a vim clutch that enters insert mode when you press down on the pedal, and leaves insert mode when you release it. Since the Teensy&#8217;s Button class detects both button press and button release events, we can write code to do that.</p>

<p>Here&#8217;s what the code looks like:</p>

<script src="https://gist.github.com/mathias/6168386.js"></script>


<p>(You can find the code in my chording keyboard repo on Github <a href="https://github.com/mathias/chording/blob/master/teensy/foot_pedal/foot_pedal.ino">here</a>.)</p>

<p>There are, of course, a few gotchas that I ran into:</p>

<p>The <code>KEY_ESC</code> constant that is referenced by the <a href="http://arduino.cc/en/Reference/KeyboardModifiers">Arduino documentation</a> didn&#8217;t work. Similarly, sending the hex value and character code didn&#8217;t work. I couldn&#8217;t find anything online that suggested that the Mac has some different ASCII character for the Escape key, so I had to find another way to leave insert mode.</p>

<p>With the Arduino&#8217;s Keyboard class, we can build up a key combo by calling <code>Keyboard.press</code> for each character in the combo, and then finally calling <code>Keyboard.releaseAll</code> when we&#8217;re ready to send the key combo to the computer. Since vim also has <code>Ctrl-[</code> as a way of leaving insert mode, I created that key combo on lines 34-36.</p>

<p>Of course, this code could be made to be even more robust by guarding against, say, inserting an <code>i</code> character when you are already in insert mode. Most likely, you&#8217;d send Ctrl-[ (and move building the key combo to a function we can reuse) and then send vim the command <code>:startinsert</code>.  I&#8217;ll leave implementing the more robust solution as an exercise for the reader.</p>

<h2>Final thoughts</h2>

<p>Does it work? I used it while writing this blog post, but I must admit, I have a strong natural reflex to hit ESC to leave insert as soon as I finish typing a word or sentence. However, when consciously trying, it is quite natural to use the foot pedal to enter/leave insert mode. One downside I found was that the Ctrl-[ combo seems to back up the cursor one character, which can be annoying.</p>

<p>For my personal use, I am thinking about mapping the key it sends to <code>Left-Alt</code> so that I can use the footpedal while in emacs&#8217; Org Mode and not have to move my left hand down to hit Alt with my ring finger every time I want to adjust a heading or start a new heading.</p>

<p>Overall, this pedal is much cheaper than the high-end pedals mentioned above. The total cost came to around $37, because I had some of the parts on hand. Further, you can upload new code to the Teensy on all operating systems, a big win over the Windows-only pedals above.</p>

<p>However, this pedal only has one foot switch. You could easily add more M-Audio sustain pedals to the design, and keep adding 1/4&#8221; jacks to a project box. The Teensy has plenty more IO lines to use! Or you could go for it all in one enclosure with the guitar pedal hardware.</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9451908041/" title="IMG_1903 by Matt Gauger, on Flickr"><img src="http://farm4.staticflickr.com/3780/9451908041_c8446aa3e7_z.jpg" width="480" height="640" alt="IMG_1903" style="display: block; margin: 0 auto;" /></a></p>

<hr />

<p>If you have any questions or comments, I&#8217;d love to hear about it over on Twitter, where I am <a href="https://twitter.com/mathiasx">@mathiasx</a>.</p>

<h3>Research:</h3>

<ul>
<li><a href="http://hackaday.com/2012/06/21/building-a-clutch-for-vim/">Hackaday: Building a clutch for vim</a></li>
<li><a href="http://arduino.cc/en/Reference/KeyboardModifiers">Arduino Keyboard modifiers</a></li>
<li><a href="http://www.emacswiki.org/emacs/FootSwitches">Emacs Wiki: Foot Swtiches</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a chording keyboard: progress so far]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/08/03/building-a-chording-keyboard-lessons-learned-and-progress-so-far/"/>
    <updated>2013-08-03T08:21:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/08/03/building-a-chording-keyboard-lessons-learned-and-progress-so-far</id>
    <content type="html"><![CDATA[<p>Chording keyboards have been a computing peripheral for a long time. They&#8217;ve been in use at least since Douglas Engelbart gave what is known as <a href="http://www.youtube.com/watch?v=yJDv-zdhzMY">The Mother of all Demos</a> in 1968. Rather than try to sum up the history and potential of the device here, I recommend you read the excellent essay <a href="http://www.loper-os.org/?p=861">Engelbart&#8217;s Violin</a> by Stanislav Datskovskiy on loper-os.org</p>

<p><img src="http://blog.mattgauger.com/images/engelkey.jpg" alt="Engelbart's chording keyboard in use" /></p>

<p><cite>Image via <a href="http://research.microsoft.com/en-us/um/people/bibuxton/buxtoncollection/detail.aspx?id=7">http://research.microsoft.com/en-us/um/people/bibuxton/buxtoncollection/detail.aspx?id=7</a></cite></p>

<p>I&#8217;ve always had an interest in building hardware. When I was younger, I dreamed of building robots some day that I could interact with, play games with, or maybe go to space with. Someday I hope that I&#8217;ll get that chance. But in the meantime, I&#8217;ve built all sorts of hardware, and I&#8217;ve always been interested in the low-level side of computing.</p>

<p>At <a href="http://bendyworks.com">Bendyworks</a> right now, we have flurry of activity and interest focusing on low-level computing, digital logic, and hardware. We&#8217;ve had several discussions in off-hours about learning computing from first principles: digital logic, circuit design, and taking that knowledge to the point of designing and building CPUs and then computers around those novel CPUs. There&#8217;s interest in the <a href="http://arduino.cc/en/">Arduino</a> and some of my coworkers are taking the so-called <code>nand2tetris</code> course: <a href="http://www.nand2tetris.org/">Building a Modern Computer from First Principles</a>.</p>

<p>We recently built a project at Bendyworks that we call <a href="https://github.com/bendyworks/concert_cam">concert_cam</a>, which takes pictures at the <a href="http://www.liveonkingstreet.com/">Live on King Street concerts</a> outside our office. You can see a <a href="https://www.facebook.com/media/set/?set=a.180416672130576.1073741826.180178628821047&amp;type=3">gallery the pictures taken at the most recent concert</a> on Facebook. At some point I&#8217;ll blog about the <code>concert_cam</code> project and provide some lessons learned.</p>

<p>But for now, I&#8217;d like to talk about a project I&#8217;ve been working on for awhile: a chording keyboard. I don&#8217;t have a clever name for it, so the code currently lives at <a href="https://github.com/mathias/chording">github.com/mathias/chording</a>. The repo represents only some of the attempts I&#8217;ve made at getting it to work. (Code from several other attempts to code a chording keyboard died in a VMware Linux VM that ate its own disk image, and I didn&#8217;t commit any of that code. Luckily, that code was mostly things I&#8217;d ruled out as possible solutions.)</p>

<p>Building a chording keyboard in software is not trivial, so I have moved on to building one purely in hardware.</p>

<div id="background-section"></div>


<h2>Background information</h2>

<p>USB devices that we use for input, like mice and keyboards, implement something called USB HID, for Human Interface Device. Linux, Mac OSX and other operating systems have supported USB HID for a long time.</p>

<p>To build a chording keyboard that can work on any computer without special software installed, the device will have to implement USB HID and send the correct key events for a given chord.</p>

<p>There are lots of chording keyboard projects and commercial products out there. To name just a few:</p>

<ul>
<li>The <a href="http://en.wikipedia.org/wiki/Microwriter">Microwriter</a>, which the author of loper-os.org discusses in the essay <a href="http://www.loper-os.org/?p=861">Engelbart&#8217;s Violin</a>, and also <a href="http://www.loper-os.org/?p=1066">covered reverse-engineering</a>.</li>
<li>The <a href="http://www.handykey.com/">twiddler</a>, which is commercially available. I have used my coworker <a href="http://sencjw.com/">Chris</a>&#8217;s twiddler quite a bit, but couldn&#8217;t get over the TV-remote-control ergonomics.</li>
<li>The <a href="http://rhodesmill.org/brandon/projects/tabspace-guide.pdf">tabspace layout for the twidder</a> (PDF link), which is an optimized key map layout for Twiddler.</li>
<li>The <a href="http://wearcam.org/septambic/">septambic keyer</a> by Steve Mann.</li>
<li>The <a href="http://chordite.com/">chordite</a> keyboard.</li>
<li>The <a href="http://symlink.dk/projects/spiffchorder/">spaceman spiff layout / spiffchorder project</a>.</li>
<li>The <a href="http://gkos.com/gkos/index-gkos-com.html">gkos</a> project to create a software chording keyboard for smartphones.</li>
</ul>


<p>Ultimately, none of these projects or products really fit what I had in mind for a chording keyboard.</p>

<p>Why a chording keyboard, you might ask?</p>

<p>Well, I spend most of my work day editing text. While I am quite proficient at vim and slowly getting better at emacs, the kinds of key combos that a professional programmer uses daily are quite complex. There is a constant risk of RSI or carpal tunnel (which plagued me in my teens, but I have been free of for over a decade.)</p>

<p>With a chording keyboard, one could take a common key combo that you use all the time and put it under a much easier-to-type chord. This becomes especially attractive to me in replacing some of the key-combos required for operating emacs (especially those that use alt.)</p>

<p>Of course, such a device isn&#8217;t designed to completely replace having a keyboard on the desk. Even with a lot of practice, the reality is that I&#8217;ll probably continue to be faster at typing words on QWERTY. But having a chording keyboard off the left side, and a mouse on the right, seems to make sense. I am, after all, a software craftsman, and if I feel like I need to build a particular tool (even a physical piece of hardware) to augment my current toolbox, then it makes sense to do it.</p>

<p>For a long time, I have dreamed of a way of being able to record programming language keywords and idioms as macros, as well as methods to type faster. Since I haven&#8217;t found any way to train myself to type QWERTY faster, I started to look at alternatives. Learning Dvorak or Colemak might help, but it still doesn&#8217;t get me away from the legacy typewriter keyboard design.</p>

<p>And still, those alternative layouts doesn&#8217;t get me to the point where I can type out an entire block of code at once. For example, I might want to map a key to output something I frequently type, such as a Javascript anonymous function. Knowing which editor I&#8217;m in, I could have the keyboard leave the cursor in the function body, ready to be filled in:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>function() {
</span><span class='line'>  _
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>(where the underscore shown is the cursor position)</p>

<p>And lastly, I wanted to build a chording keyboard because it&#8217;s cool, it&#8217;s different, and because <strong>we frequently accept the paradigms handed to us in computing without really thinking about how we might improve or replace them.</strong></p>

<p>To understand chording keyboards best, it is worth noting that while regular keyboards detect the initial key being pressed down, a chording keyboard must detect when all of the keys in a chord are released simultaneously. Due to how fast electronics really are, no human could actually release all of the keys at exactly the same moment. For that reason, we need to implement chording in a way that takes time into account and looks to see if any chord pattern matches in the past n milliseconds.</p>

<h2>Brief project history</h2>

<p>After reviewing possible hardware to use to build a chording keyboard, I stumbled upon the Razer Nostromo (and its earlier incarnations from Belkin.) These gaming pads are popular with PC gamers because it puts the WASD keys under your left hand and frees your right hand up for the mouse.</p>

<p><img src="http://blog.mattgauger.com/images/razer_nostromo.jpg" alt="Razer Nostromo" /></p>

<p>Other features the Nostromo had over other devices were supporting <a href="http://en.wikipedia.org/wiki/Rollover_(key)#n-key_rollover">N-key rollover</a> &mdash; basically, the ability for the keyboard to know that multiple keys are being pressed at once, and send all of those keys to the computer &mdash; as well as a little 4-way directional pad, and blue lights (always a plus!)</p>

<p>The Nostromo also comes with a piece of software that you can install and remap the key mappings with. The key mappings get saved onto the device, meaning the device can operate without the software. The current mode is indicated by three LEDs near the thumb, which count up in binary, yielding 8 key maps. Additionally, any of the keys can trigger a pre-recorded macro of many key events.</p>

<p>Sadly, while the Nostromo software can have one key press trigger a sequence of buttons for you, for example, to macro a complicated action in a game, it did not support detecting a chord (multiple key presses simultaneously) into a single key event. So with that limitation in mind, I set about trying to write code to detect chords and turn them into key presses in software rather than at the hardware layer.</p>

<h3>mxk</h3>

<p>On Linux, there is a project called <a href="http://welz.org.za/projects/mxk">mxk</a>. <code>mxk</code> is basically a USB HID swiss army knife and HID event remixer, able to take input from any USB device, run it through various rules, and then output new USB HID events to the USB bus with a &#8220;virtual&#8221; USB HID device. Many example configuration scripts are provided, and one of the most common use seems to be to create a key press that turns a QWERTY keyboard into a DVORAK keyboard and back again, without having to change the Linux system settings. It seemed promising.</p>

<p><code>mxk</code> even indicated that it had support for two different kinds of chording: braille keyboard chording and a simpler chord-matching function.</p>

<p>After banging my head against its confusing configuration syntax for about 2 weeks, I determined that <code>mxk</code> just couldn&#8217;t do what I wanted. Two issues cropped up: in the braille chording function, only one row of keys was supported and two hands were assumed. (Basically, it wanted ASDF and JKL; keys to form chords, and didn&#8217;t support anything else.)</p>

<p>The regular chording functionality seemed like it might work, until I ran into a huge, glaring issue: A key used in one chord couldn&#8217;t be used in another. For example, let&#8217;s say I&#8217;m mapping keys for the left hand on a QWERTY keyboard to be used in a chord. So I might have a chord like RE which maps to &#8216;a&#8217; and another chord like FE that maps to &#8216;k&#8217;. The E used in both chords didn&#8217;t work in <code>mxk</code>. It simply couldn&#8217;t support it; it could only watch for unique chords. This limits the key map for a chording keyboard made of a 12-key grid from some 495 combinations to 12, gaining us nothing since individual key presses are just as efficient.</p>

<p>With these limitations discovered, I tried to hack the code for <code>mxk</code> to support my use case better. But, I don&#8217;t have much in the way of C chops when it comes to device drivers and pointers, and implementing code against Linux&#8217;s libHID is kind of a pain. Further, the <code>mxk</code> source isn&#8217;t very well documented, outside of very few comments littered around the code. Additionally, I found libHID&#8217;s docs unapproachable.</p>

<p>I realized that I was <strong>working at the wrong abstraction level</strong>, like so many projects that had frustrated me before. So I decided to move up to something at a higher level, with the hope that someone else had figured out the headaches of low-level libHID implementation. There are many high level libraries built on libHID. But since there isn&#8217;t a whole lot of need for them in most computing, they tend to be abandoned, very old, simplistic, or all of the above. I also was frustrated that my attempts so far would lock me into only using a Linux machine, as my normal work environment at Bendyworks is Mac OSX. We pair all of the time, so I would not be able to use my chording keyboard whenenver I paired with someone else.</p>

<h3>Plover</h3>

<p>Another project that I found that seemed promising was <a href="https://github.com/plover/plover">Plover</a>. At first glance, Plover doesn&#8217;t seem to be the right kind of software at all. It is an open source stenographer program that allows transcriptionists and court stenographers to turn a regular QWERTY keybord with n-key rollover into a high-end steno tool. The specialized hardware that court stenographers use can cost thousands of dollars, so an open source project that implements it for free on a $100 keyboard is a great win for those in that field.</p>

<p>Despite coming mapped for stenography and transcription &mdash; a system which I find very confusing &mdash; at its heart, Plover is a cross-platform Python GUI app that supports turning chords into words or phrases. This seemed like exactly what I needed. Going through the source code, I noticed that different keyboards got different configuration classes, and set about trying to create a class to map the keys on the Nostromo to the key events that Plover was expecting.</p>

<p>Now Plover comes with a huge dictionary of words that chords map to. And those chords are made up of certain key events, but not the ones we&#8217;re used to. (Some idea of how they look can be found <a href="https://github.com/plover/plover/blob/master/plover/machine/sidewinder.py#L14-L51">here</a> on Github.) How those special events combine and whether they come at the beginning or end of the chord seems to be the most important part of understanding this system. I struggled to understand the mappings, but it was pretty complicated. Plover comes with a giant, 124,000+ line JSON dictionary. I threw that out and started trying to write my own JSON dictionary; a humbler one that simply mapped chords to key events and a few Ruby &amp; JavaScript programming idioms.</p>

<p>I struggled with understanding the key event names and how they combined (since I don&#8217;t have a background in stenography) and eventually, lost the work due to the VMware Linux VM crash mentioned above. Taking it as a sign that I should not continue down the route of using Plover, I put aside the chording keyboard project for many months.</p>

<h3>Teensy and a return to hardware</h3>

<p>Originally I had tried to avoid hacking hardware so that I could focus on what I do day-to-day: write software. But in the end, I felt like I had exhausted my ability to get this project off the ground by only writing software.</p>

<p>When we built the <code>concert_cam</code> project at Bendyworks, the first version of the button pedestal we built utilized a <a href="http://www.pjrc.com/teensy/">Teensy board</a> &mdash; basically, a small Arduino-like microcontroller. The huge feature that the Teensy provides over other Atmel boards is that it comes out-of-the-box with a firmware that sets it up as a USB HID device.</p>

<p>The basic process for programming and using the Teensy is this:</p>

<ul>
<li>You program it in the Arduino IDE like other Arduino boards</li>
<li>Anything you &#8220;println&#8221; will be sent as if someone had typed it on a USB keyboard.</li>
<li>Additionally, there is a Mouse class that you can use to send mouse movement, click, and scroll wheel information.</li>
<li>When you upload your code to the Teensy, it reboots, and immediately starts behaving like a USB HID device as far as the operating system is concerned.</li>
</ul>


<p>For the <code>concert_cam</code>, it was easy enough to have a big arcade-style button send the keyboard event of &#8220;p&#8221; (for press) to the Raspberry Pi that the Teensy was attached to. A script on the Raspberry Pi watched for this keyboard input and told the camera to take a picture. It was very quick to get that working.</p>

<p>Here&#8217;s the <code>concert_cam</code> button pedestal with Teensy and Raspberry Pi getting wired in:</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9427820477/" title="IMG_1817 by Matt Gauger, on Flickr"><img src="http://farm8.staticflickr.com/7395/9427820477_a8b4630519_z.jpg" width="480" height="640" alt="IMG_1817"></a></p>

<p>Confident by my success with using the Teensy in the <code>concert_cam</code>, I decided to put a Teensy in my Nostromo, replacing the normal USB board inside it and translating keyboard chords into key press events at the hardware level.</p>

<p>The first step is to open up the Nostromo. This is pretty easy; just unscrew all the visible screws, and find &amp; unscrew the other screws underneath the rubber feet of the gamepad. The Nostromo comes apart into three pieces, and looks like this inside:</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9427844525/" title="iPhoto by Matt Gauger, on Flickr"><img src="http://farm4.staticflickr.com/3784/9427844525_4b59fa4bc3_b.jpg" width="581" height="733" alt="iPhoto"></a></p>

<p>The first problem encountered is those little grey ribbons - they&#8217;re ZIF surface mount stuff, and everything else inside is surface mount here, too. For an electronics hobbyist, surface mount components can represent a big source of frustration. For me, I just didn&#8217;t want to deal with them. I can&#8217;t really solder to the cables with any precision and the spacing of the connectors was so small that there was no hope of attaching wires there. Further, the circuit boards had traces running inside but didn&#8217;t really have any pins that I could solder to. A real bummer.</p>

<p>I thought that maybe I could get more of the plastic cable connectors (the little white and black connectors on the main circuit board), but with a regular pin spacing, not surface mount. All attempts to find such a part were unsuccessful. So I had to get creative.</p>

<p>The Nostromo uses membrane keys, which most laptop and PC keyboards nowadays do. A long time ago, almost all keyboards used &#8220;mechanical&#8221; key switches &#8211; switches that had springs or other tension devices and sent a keypress when continuity was made between two contacts. The loud, much-loved IBM Model M keyboard counts as a mechanical keyboard. Gaming keyboards and a few hardcore programmers (like many of us at Bendyworks) are holdouts for using mechanical keyboards, and a few companies still make them. For mechanical key switches, the current winner is the Cherry MX line. (To learn more about mechanical keyboards and Cherry MX switches, I recommend <a href="http://www.wasdkeyboards.com/mechanical-keyboard-guide">this guide</a>.)</p>

<p>Cherry MX key switches are not all that expensive, and I&#8217;d only need 15 to convert the Nostromo. I put in an order for the Cherry MX browns from <a href="http://www.wasdkeyboards.com/">wasdkeyboards.com</a> and they shipped them out fast. I figured I could get away with using the keycaps from a number pad, which are also cheap, and so I threw those into the order as well.</p>

<p>However, by replacing the key switches, I was getting into a realm of hardware hacking that I was trying to avoid: fabbing physical things from scratch. For this project, I wanted the keyboard to be sturdy and reliable; I didn&#8217;t want it to fall apart or move under my hand or rattle. Making a mounting plate that they keys snap into is the typical way that hobbyists build their own mechanical keyboards. And lots of people do, over on sites like <a href="http://deskthority.net/">deskthority.net</a> and <a href="http://geekhack.org/">geekhack.org</a>, but without a really well stocked workshop or a Makerspace membership, I didn&#8217;t want to go down that road.</p>

<p>Luckily, some quick googling turned up a <a href="http://www.xim3.com/community/index.php?PHPSESSID=98v9639t6etse5msjbupuogp24&amp;topic=8122.0">forum post in which the author replaces Nostromo key switches with Cherry MX keys</a>. The mounting plate for the membrane keys in the Nostromo requires only a little dremeling to convert. Since I was also throwing out the existing USB circuit board for the Nostromo, I can get away with removing the membrane switch PCB and soldering a cable directly to the Teensy&#8217;s pin headers, getting rid of those pesky ZIF cables.</p>

<p>Currently, I&#8217;m waiting for the key switches to arrive so that I can cut down the white mounting plate for each switch. My plan is to wire up each key to an IO line on the Teensy. Typically, keyboards use multiplexing in a grid to let the computer know which key was pressed: essentially, rows of keys are connected to one set of pins and columns of keys are connected to another set of pins, and when a pin for a row and a column goes HIGH, you know which key was pressed based on it being the intersect of that row and column. The issue you run into here is, again, complicated by chording. While the microcontroller can figure out reasonably well individual keys, there are issues on some keyboards with &#8221;<a href="http://www.microsoft.com/appliedsciences/antighostingexplained.mspx">ghosting</a>&#8221; other keys when chording.</p>

<p>Since I only have about 15 keys and maybe another 4 or 5 input/outputs that I need, I can get away with using separate IO lines on the Teensy 2.0. The Teensy++ 2.0 has are even more IO lines, which would make it better suited to projects with more IO lines needed. Not needing diodes to ground, not having to wire up a matrix, and not having to detect two pins makes both the hardware and software here much simpler.</p>

<h3>Scroll wheel</h3>

<p>While waiting for the key switches to arrive, I began work on the one non-surface-mount component in the Nostromo that I could solder to: the scroll wheel. On the scroll wheel&#8217;s PCB, there is a mechanical encoder, a button (the click when you press down on a mouse scroll wheel) and an LED.</p>

<p><a href="http://www.flickr.com/photos/mattgauger/9430775972/" title="skitched-20130803-121011 by Matt Gauger, on Flickr"><img src="http://farm8.staticflickr.com/7453/9430775972_991c8e8a8d_c.jpg" width="600" height="800" alt="skitched-20130803-121011"></a></p>

<p>Here&#8217;s a quick link for reading up on how mechanical encoders work in mice, and particularly, how to interface one with an Arduino: <a href="http://forum.arduino.cc/index.php/topic,15336.0.html">Mouse Scroll Wheel Sensor and Arduino</a>. There&#8217;s also some source code in that thread that will probably prove useful later. Note in the picture that the middle leg of the encoder is ground, and the two ends correspond to the signals sent when you rotate it &mdash; you&#8217;ll have to figure out whether the scroll direction matches their location yourself by experimenting.</p>

<p>I used part of my Friday Growth Day at Bendyworks to start wiring up and coding the firmware for the Nostromo&#8217;s scroll wheel. The basic idea is that the mechanical encoder has two outputs. You monitor both lines on separate IO pins on the Teensy, and whichever IO line goes HIGH first is the direction of the scroll. With the Teensy&#8217;s convenient Mouse class, you can then just send a scroll event to the OS. In a matter of minutes, I had my scroll wheel scrolling web pages. Not bad.</p>

<p>I&#8217;ll be implementing a variation on the sliding buffer for key events, so I will probably also use that code for handling scrolls (over a much shorter, separate buffer.)</p>

<p>Since I was trying to understand the mechanical encoder and could only really connect alligator clips to one pin at a time, I wrote up a quick Arduino script to detect only one direction of scrolling. Here&#8217;s that code:</p>

<script src="https://gist.github.com/mathias/6147169.js"></script>


<p>In my repo on Github: <a href="https://github.com/mathias/chording/blob/master/teensy/scroll_wheel/scroll_wheel.ino">scroll_wheel.ino</a></p>

<p>Note that this does not implement a sliding buffer or direction detection.</p>

<p>Eventually, I&#8217;d like to break my scroll wheel code out into an Arduino library that takes the two input pin numbers and a velocity as parameters to the constructor, and implements the scroll direction detection. The library would send the scroll event for you based on the velocity you passed in. But that will have to come later.</p>

<h3>Future work</h3>

<p>I&#8217;m going to be getting the key switches in the next week and beginning the hardware hacking to install them.</p>

<p>On the software end, I&#8217;d like to write a script (probably in Ruby) that takes a standardized keyboard mapping file and converts it into a C++ header file that the Arduino can utilize for its chord mappings. That way, I don&#8217;t have to maintain the header file manually or know what C++ constants map to what, I will simply edit my simple key mappings file and regenerate the code.</p>

<p>The <a href="https://github.com/mathias/chording/blob/master/twiddler_keymap.txt">twiddler_keymap.txt</a> file in the repo represents the current plan for my keymap. It is based on the <a href="">Tabspace layout</a> mentioned above in the <a href="#background-section">Background</a> section. I haven&#8217;t found any reason to not use the Tabspace layout, as it seems sensible and leaves plenty of room in the unmapped chords for me to implement key combos, programming language idioms, and text editor movement.</p>

<p>One last decision I need to make is whether to replace the Nostromo&#8217;s thumb directional pad. The current thumbpad uses membrane switches similar to the keys, and is all surface mount with very little access to its signals. One thought is to replace the 4-way pad with a Playstation-controller style <a href="https://www.sparkfun.com/products/9032">joystick</a> from Sparkfun.</p>

<p><a href="https://www.sparkfun.com/products/9032">
<img src="http://blog.mattgauger.com/images/sparkfun_joystick.jpg" alt="Sparkfun thumb joystick" />
</a></p>

<p>With this joystick, I&#8217;d be able to implement either mouse movements or arrow key movements, and could probably toggle which it behaves like with the small button located above the directional pad. However, securely mounting the joystick into the Nostromo case so that it is durable might prove an issue. I&#8217;ve put off making this decision until I have successfully hacked the mechanical switches in.</p>

<p>I&#8217;ll be blogging about each step of finishing this chording keyboard as I go. Stay tuned for the next post about installing up the key switches and wiring them to the Teensy.</p>

<h3>Final thoughts</h3>

<p>I&#8217;ve got a lot of projects, and I put them down regularly to focus on something else. That list of projects continues to grow and grow. So I don&#8217;t feel very guilty that I&#8217;ve put down this project for awhile and am just now picking it up again. I&#8217;m making good progress, and find it interesting rather than frustrating, so I will continue work on it.</p>

<p>In truth, this project doesn&#8217;t really represent much value outside of the things I&#8217;m learning as I go. I have no ambitions of turning this into a commercial product, and while some people might find my notes here useful, I&#8217;m not trying to make a repeatable project that others can build part-for-part. Is it worth it? Definitely. Is it for everyone? Probably not.</p>

<p>One thing I&#8217;d like to do after completing this device is begin work on a custom computer. Specifically, I want to learn enough digital logic concepts to design a whole computer in a language like Verilog or VHDL, and then burn the design into a FPGA board.</p>

<p>Such a computer would <strong>not</strong> be intended to compete with your Core i7 quad core &#8211; I&#8217;m thinking of implementing a unique architecture with the overall processing capability of something like an early 80&#8217;s microcomputer. It would be interesting, but not entirely useful for day-to-day computing.</p>

<p>When the computer is built, I&#8217;d begin implement an operating system on top of that computer in Lisp, along the lines of what the author of <a href="http://loper-os.org">loper-os.org</a> has been working towards for many years.</p>

<p>Such a project may take a decade or more to realize, though, and so is not undertaken lightly or easily accomplished.</p>

<hr />

<p>If you have any questions, comments, or your own chording keyboard project, I&#8217;d love to hear about it over on Twitter, where I am <a href="https://twitter.com/mathiasx">@mathiasx</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[By augmenting human intellect, we mean]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/03/17/by-augmenting-human-intellect/"/>
    <updated>2013-03-17T22:44:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/03/17/by-augmenting-human-intellect</id>
    <content type="html"><![CDATA[<blockquote>
By &#8220;augmenting human intellect&#8221; we mean increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems. Increased capability in this respect is taken to mean a mixture of the following: more-rapid comprehension, better comprehension, the possibility of gaining a useful degree of comprehension in a situation that previously was too complex, speedier solutions, better solutions, and the possibility of finding solutions to problems that before seemed insoluble. And by &#8220;complex situations&#8221; we include the professional problems of diplomats, executives, social scientists, life scientists, physical scientists, attorneys, designers&#8211;whether the problem situation exists for twenty minutes or twenty years. We do not speak of isolated clever tricks that help in particular situations. We refer to a way of life in an integrated domain where hunches, cut-and-try, intangibles, and the human &#8220;feel for a situation&#8221; usefully co-exist with powerful concepts, streamlined terminology and notation, sophisticated methods, and high-powered electronic aids.
<footer>
<strong>Douglas C. Engelbart</strong>
&ndash;
<cite><a href="http://www.dougengelbart.org/pubs/augment-3906.html">Augmenting Human Intellect: A Conceptual Framework </a></cite>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Filter or Be Filtered]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/03/07/filter-or-be-filtered/"/>
    <updated>2013-03-07T22:09:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/03/07/filter-or-be-filtered</id>
    <content type="html"><![CDATA[<p>Eli Pariser&#8217;s talk, <a href="http://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles.html">Beware online &#8220;filter bubbles&#8221;</a> recently hit the front page of the TED Talks website. I found it interesting because it discusses some of what I&#8217;ve seen happening online, and some of my fears for how we search and consume content. Watch the video if you haven&#8217;t to see just how much is already being filtered on the web for you.</p>

<p>Right after I saw the video, of course, <a href="http://washingtonpost.com/business/technology/facebook-to-change-news-feed-to-a-personalized-newspaper/2013/03/07/b294f61e-8751-11e2-98a3-b3db6b9ac586_story.html">Facebook announced</a> that they&#8217;ll be rolling out a new version of the the news feed. They are quoted as making it more like a &#8220;personal newspaper&#8221; &mdash; a filter, essentially, for the things Facebook thinks you want to see. As the video above pointed out, they&#8217;ve already been filtering <em>out</em> the things they think you don&#8217;t want for awhile.</p>

<p>No doubt there will be some backlash as people adjust to changes on Facebook, but very few will abandon it. And if you are the kind of person that would abandon Facebook over filtering, or privacy, or concerns about owning your own data, you don&#8217;t have a lot of choices of where to go.</p>

<p>Sure, you can specifically seek out services that aren&#8217;t going to give you a filtered world view &mdash; <a href="https://duckduckgo.com/">DuckDuckGo</a> comes to mind. But that&#8217;s not always possible. The problem is bigger than just the search engine you use and whether or not you&#8217;re logged into Facebook. Just about every site is run by someone else, and they are analyzing you constantly. They are in a brutal battle to keep you as their consumer and keep you from going to other sites. But are the things that they think you want to see what you really want or need to see?</p>

<p>As a developer, I&#8217;m capable of taking matters into my own hands, to some degree. Most of the sites that I use on a daily basis have an API and allow me to export or scrape my personal data. But the cost of switching off of the nice service, the well-designed UI/UX of an official mobile app, and so on has kept me from taking the plunge to export my data and go set up shop on my own version of various web services.</p>

<p>Recently I revamped a very old, empty repo that I had on Github. The point of this repo was code a way to export all of my data from <a href="https://www.last.fm/">Last.fm</a>. But for whatever reason the repo has been empty for something like 4 years now. So the other week, I sat down, checked the <a href="http://www.last.fm/api">Last.fm API docs</a>, and wrote a very simple Ruby script to dump out all the JSON it could about my top tracks, artists, and albums over the past 7 years. That script is available as my <a href="https://github.com/mathias/birdsong">birdsong</a> repo on Github.</p>

<p>So far, I don&#8217;t have a real use for the data. I could try to use some visualization tools on it to make cool graphs or maps. Or I could try and get analytics out of it: genres listened to, how they&#8217;ve changed over the years. But for now, I am content to just have the JSON data.</p>

<p>All in all, it&#8217;s around 30 megabytes of JSON data, which is really just plain text with no compression, so it is really quite a lot of data. There&#8217;s more I can do, though. I plan to do to start taking control, to both aggregate my own data and filter it.</p>

<p>For a long time it has bothered me that <a href="https://www.google.com/reader">Google Reader</a> stopped receiving new features, and the features that existed only went so far. To be fair, Google Reader has been a rock solid web service for many, many years for me. It allows me to read my feeds quickly, reliably, and has parsed feeds well &mdash; all problems I&#8217;ve had with other feed readers. It&#8217;s always had some neat features like showing me analytics of what I read.</p>

<p><img src="http://blog.mattgauger.com/images/google_reader_trends.png" alt="Google Reader Trends" /></p>

<p>But as far as discovering new content and helping me to eliminate content I don&#8217;t read or don&#8217;t want to read, Google Reader is not so great. Reader is terrible at suggesting new feeds to read to me; it constantly suggests Dilbert and Lifehacker, and has for the past 5 years or so. I have no interest in either of those. It has never analyzed my reading patterns to such a degree that it suggested something new that blew me away. I am looking for those kinds of interesting suggestions: not just for feeds that match what I already read, but things that are outside of my normal bubble but would be interesting to me.</p>

<p>Luckily, Google Reader has an API, and rather than just exporting my data and building a new service, I can start to build off of it. I get to keep a lot of the features I enjoy while extending it with my own code.</p>

<p>In Pariser&#8217;s talk, he talks about encoding algorithms of filtering and recommendation with a sense of civic duty. To some degree, this means having some journalistic integrity. Such an algorithm needs to present both sides of the story. One issue of many feed readers and other content online is that, well, it only shows one view. The view of the article you&#8217;re on.</p>

<p>But imagine a feed reader that was more like the front page of Google News. It would not only show you the blog post you&#8217;re reading, but all recent blog posts from other authors about similar topics. Maybe, if they&#8217;re responding to a news event or writing about a known fact, the reader could do the work to track down the original source. To take it even further, without even really needing to be aware of motive, politics, and other factors, a dumb feed reader with good suggestions could probably present both sides of the story. Both sides of an argument. Both liberal and conservative takes on the same bill.</p>

<p>Dreaming up features like this can be a deep rabbit hole. Start considering the consequences of pulling up all past articles you&#8217;ve read about similar keywords or tags. Or performing searches for academic papers on the topic. Pulling in data from Wikipedia. Looking up books you&#8217;ve read or are planning to read on Goodreads that are related to the blog post you&#8217;re reading. Or any other number of ways to slice and dice content. And it doesn&#8217;t have to stop with Google Reader.</p>

<p>Opportunities to make better tools and more intelligently consume information are all around us. At the same time, consumer-consuming corporations on the web try to trap us into filter bubbles. They try to provide us with what they think we want, but can they ever really know? In the end, it&#8217;s control-your-own-filters or be filtered<a href="#filter-or-be-filtered-note" name="filter-or-be-filtered-note-return"><sup>1</sup></a>.</p>

<hr />

<p><a name="filter-or-be-filtered-note"></a></p>

<p><strong>1</strong> The name of this article was lifted from Daniel Rushkoff&#8217;s <a href="http://www.goodreads.com/book/show/9408311-program-or-be-programmed">Program or be Programmed</a>, a book which I didn&#8217;t really enjoy. It was not what I thought it would be about: why we should learn to program so that we can control the complex technical systems around us rather than be controlled by them, or how programming&#8217;s problem solving skills can be applied elsewhere. Instead, I found the book to be some technology-fear-mongering and a bunch of diatribes about how things online or in computers are &#8220;less real.&#8221; Suffice to say, I did not enjoy it. <a href="#filter-or-be-filtered-note-return">&#8617;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Insanely powerful widgets, brought to you by Moore's Law]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/03/03/insanely-powerful-widgets-brought-to-you-by-moores-law/"/>
    <updated>2013-03-03T11:10:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/03/03/insanely-powerful-widgets-brought-to-you-by-moores-law</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been teaching my downstairs neighbor basic electronics and how to solder. He&#8217;s a musician and has been working his way through several kits from <a href="http://bleeplabs.com/">Bleep Labs</a>.</p>

<p>As I explained each component: the resistor, the capacitor, the diode, etc., I eventually got to the transistor. I told my neighbor how transistors are rarely used on their own now. The transistors in the kit were there mainly because they were easy to solder, but usually a circuit designer would opt not to use them.</p>

<p>On its own, a single transistor can&#8217;t do much, and takes up some amount of space, which is actually quite large relative to the circuits we build now. I scored bags of hundreds of transistors that were about to be thrown out at the <a href="http://milwaukeemakerspace.org/">Milwaukee Makerspace</a> when we were putting together the electronics lab &mdash; not because they didn&#8217;t work but because that many transistors simply wouldn&#8217;t get used. &#8220;Nowadays, you might as well throw an Arduino in a project,&#8221; was one of the reasons. No one wants to work at the abstraction level of single transistors anymore.</p>

<p>It turned out that the <a href="http://bleeplabs.com/store/the-bleep-drum/">drum machine kit</a> that we were putting together contained an Atmel AVR microprocessor - the main chip of the Arduino. In an effort to save space and complexity, the designers had used a microprocessor instead of discrete transistors. The chip in the Arduino and the Bleep Drum is the ATmega328, which has something like 600,000 transistors inside it.
And I explained how the form factor of the components we were using was obsolete. Even the tight-packed pins of a typical integrated circuit (a computer chip, in common parlance), spaced at 0.1&#8221;, just isn&#8217;t dense enough for modern circuits. The parts we were using were all designed to be soldered to the circuit board by human hands. Slow, error-prone human hands. The vast majority of circuit boards produced today are <a href="http://en.m.wikipedia.org/wiki/Surface-mount_technology">surface mount</a>: parts placed by robots and soldered all in one go by another machine.</p>

<p>&#8220;It&#8217;s really neat that you know all this stuff,&#8221; my neighbor remarked. But I shrugged it off. This knowledge, especially of how analog circuits work and how audio signals are modified by analog components, is mostly obsolete. Better to just use some analog-to-digital converters and put a microprocessor on it. Or better yet, something way more powerful than a simple microprocessor.</p>

<p>We keep putting more and more transistors on a single die, or chip, every day. This trend was first spotted by Intel co-founder Gordon E. Moore, and so we call it <a href="http://en.wikipedia.org/wiki/Moore's_law">Moore&#8217;s law</a>. So far, Moore&#8217;s prediction that the number of transistors on integrated circuits would double every 2 years has been quite accurate. It has led us from the simplest integrated circuit in 1958 to the Core i7 processor in my Macbook Air today.</p>

<p>The other day I tweeted about the crazy processing power that we get today under Moore&#8217;s Law.</p>

<blockquote class="twitter-tweet" align="center"><p>Crazy future; my MBA doesn&#8217;t sweat when running Linux-in-VMware, Chrome, Emacs, LightTable, other apps, terminals &amp; 4 REPLs all at once.</p>&mdash; mathiasx (@mathiasx) <a href="https://twitter.com/mathiasx/status/307668858985140225">March 2, 2013</a></blockquote>


<script async src="http://blog.mattgauger.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>And in truth, I was running more software than I could fit into that tweet, because I was listening to music, had a few PDFs in the background, and was managing my ebook collection at the same time. But those notorious CPU hogs listed in the tweet should be enough to illustrate just how much is going on without really pushing the processing cores of the Macbook Air.</p>

<p>Remember, my 13&#8221; Macbook Air only weighs 2.96 pounds. In 2005, I had a 12&#8221; Powerbook G4 that weighed 4.6 pounds and had, by my back-of-the-envelope calculations on Geekbench scores <sup><a href="#geekbench-notes" name="geekbench-notes-return">1</a></sup>, my Macbook Air is almost 9 times as powerful as the Powerbook G4 was.</p>

<p>Where is this progress taking us? Well, my iPhone is already pretty powerful. It&#8217;s hard to get a raw number of FLOPS (floating-point operations per second) for the processor in an iPhone 4S, but we can get a Geekbench score for it. It is weighed against the same scale as the Macbook Air is measured on. On that scale, my iPhone measures<sup><a href="#geekbench-iphone-notes" name="geekbench-iphone-notes-return">2</a></sup> very close to the Powerbook G4 I had 8 years ago, but the iPhone has the advantage of having two cores.</p>

<p>Some people incorrectly read Moore&#8217;s Law as &#8220;processor <em>speed</em> doubling every 2 years,&#8221; which it is not. We&#8217;ve found that there&#8217;s more to processing power than just gigahertz, though. The number of cores in a processor, and therefore the number of simultaneous things that a computer can do, is increasing. While there&#8217;s some issues as we grow into this new paradigm where everything has multiple cores and we have to ensure consistency between them, this is overall a net win for those of us seeking more powerful computers.</p>

<p>To be honest, I don&#8217;t really even notice my computer as physical hardware anymore. It is just the stage for software to run on: quiet, fast, lightweight, and very infrequently does the hardware bog down to make me wait. Only a few short years ago, we would have to wait for the computer to do something &#8211; usually shown as the hourglass on Windows and the spinning beachball on Macs. Even things like copying a file between two directories could stop a system in its tracks, and the computer would simply stop accepting any input from the user. Now, processors, RAM, and SSDs are so fast that I rarely have to wait for them. And if I do have to wait, say, for some piece of software to be installed, the fact that the computer is multicore means that I can go and browse the internet in another window without noticing.</p>

<p>It&#8217;s likely that, just like the example of VMware running Linux running on my Macbook Air in the my tweet, eventually we will get to the point of having so much computing power embedded around us that we will virtualize everything. It&#8217;s possible that we will take not just files or even processes between computing devices but the whole environment, a whole virtualized machine and operating system. As a thought experiment, imagine pausing a VMware virtual machine while it is performing some CPU-intensive task on your laptop. Now copy that paused VM over to another machine, and start it back up. The software will continue to chug along on the CPU-intensive task like nothing happened<sup><a href="#vm-notes" name="vm-notes-return">3</a></sup>. Advances in processing power, storage speed, and wireless networking speed will continue to progress until this process could be seamless to the user.</p>

<p>In some regards, sending the whole VM across the wire and starting it back up on another processor is simpler than trying to marshal a raw process between two machines, and ensure that the process can still run in the other machine&#8217;s environment. It doesn&#8217;t solve the problem of running one process on many machines at once, but it could be used in cases where we can spin up one copy of a process that we want to parallelize, and then copy that VM to many different machines with chunks of the dataset to process. In fact, that&#8217;s basically how many large distributed processing projects like BOINC (the software that SETI@Home runs on) work.</p>

<p>And so it&#8217;s likely that, for reasons of ease of use, sandboxing for security and safety, and simply because we have so much processing power, in the future our phones and our wearable computers will simply be running a virtual machine. Then we don&#8217;t need to worry about what software is running on our desktop PCs when we get home and have access to a larger display; we just move our processing over to the desktop computer, which ever is more convenient. (Even more likely is that we just have a display that acts like an accessory that we can &#8220;throw&#8221; the video onto, rather than having a separate desktop computer.)</p>

<p>The things around us are getting progressively more powerful by way of cheap, small processors. Where before I was saying that electronics hobbyists would rather use an Arduino into a project than deal with many transistors, industry would rather throw a small ARM processor into everything around us and write software than design a custom piece of hardware.</p>

<p>Case in point, over on the Panic blog, the case of <a href="http://www.panic.com/blog/2013/03/the-lightning-digital-av-adapter-surprise/">The Lightning Digital AV Adapter Surprise</a>. After wondering why the new Lightning AV adapter for the iPad mini took a few moments to boot up, and seemed to display a scaled version of the video, they cracked open the Lightning AV cable to find: an ARM processor! As far as they can tell, the iPad mini sends a bit of software to the processor in the Lightning cable every time it is connected, essentially booting the cable up, and sets an Airplay stream down the cable to the other end, where the ARM processor decodes it, upscales it to HD, and sends it to the TV. This makes the Lightning AV cable, in essence, the world&#8217;s smallest AppleTV. And we thought the new AppleTV was small when it was debuted.</p>

<p>When I go to a concert now, the room is full of smartphones being pointed at the stage taking video or pictures. And while most people might think, &#8220;That&#8217;s a lot of pictures and video that will be uploaded to Facebook,&#8221; I think about the fact that the people in the room with me are holding more processing power in their hands than we had in most of the computer labs in my schools and in college. Those phones will just keep getting more powerful, and soon they&#8217;ll match the performance of the Macbook Air I&#8217;ve got in my backpack. Even the mundane things will have plenty of processing power because it will be cheap and simpler to put a cheap processor in it. But those cheap processors are getting more powerful every day, and that is exciting.</p>

<p>So while I may have learned digital logic in college and can help you build up a simple adder from NAND gates, and as a hobbyist I&#8217;ve learned to build <a href="http://www.geofex.com/article_folders/fuzzface/fffram.htm">guitar fuzz pedals from a few transistors</a>, that knowledge is increasingly obsolete in the face of rapid progress. We will soon be packing powerful processors into everything around us, sometimes in surprisingly ways, simply because it is easier and cheaper than designing a custom piece of hardware. But the future is exciting, and that knowledge isn&#8217;t completely useless, as long as I can share it with a few more people to show us just how far we&#8217;ve come.</p>

<hr />

<p><a name="geekbench-notes"></a></p>

<p><strong>1.</strong></p>

<p>&#8220;Geekbench scores are calibrated against a baseline score of 1,000 (which is the score of a single-processor Power Mac G5 @ 1.6GHz). Higher scores are better, with double the score indicating double the performance.&#8221;</p>

<p>Scores:</p>

<ul>
<li><a href="http://browser.primatelabs.com/geekbench2/1713385">MacBook Air (13-inch Mid 2012)</a>: 7675</li>
<li><a href="http://browser.primatelabs.com/geekbench2/1545796">PowerBook G4 (12-inch 1.5 GHz)</a>: 861</li>
</ul>


<p><a href="#geekbench-notes-return">&#8617;</a></p>

<p><a name="geekbench-iphone-notes"></a>
<strong>2.</strong> Geekbench score for <a href="http://browser.primatelabs.com/ios-benchmarks">iPhone 4S</a>: 651 <a href="#geekbench-iphone-notes-return">&#8617;</a></p>

<p><a name="vm-notes"></a>
<strong>3.</strong> Of course, there&#8217;s some issues with this. Anything that was happening synchronously would probably failed, as well as anything that depends on a network connection in progress. But for the purposes of the thought experiment, let&#8217;s ignore those problems. <a href="#vm-notes-return">&#8617;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Google Glass: You can't control the future]]></title>
    <link href="http://blog.mattgauger.com/blog/2013/03/02/google-glass-you-cant-control-the-future/"/>
    <updated>2013-03-02T15:09:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2013/03/02/google-glass-you-cant-control-the-future</id>
    <content type="html"><![CDATA[<p>There&#8217;s been quite a bit of backlash surrounding the new Google Glass product. As someone who has wanted wearable computing for awhile, I&#8217;d like to talk about it a bit. My only real qualifications on this subject come from the fact that I&#8217;ve read a lot of scifi<sup><a href="#accelerando-note" name="accelerando-note-return">1</a></sup> and that I&#8217;m an open source developer that would like to build software for wearable computing that makes people&#8217;s lives better.</p>

<p>When I&#8217;m talking about wearable computing glasses in general rather than just the Google Glass product, I will call them smart glasses. There doesn&#8217;t seem to be a consistent name yet, and the phrase <a href="http://en.wikipedia.org/wiki/EyeTap">eyetap</a> only refers to a very specific type of smart glasses.</p>

<h2>The promise</h2>

<p>As Amber Case explains in this TED Talk, <a href="http://www.ted.com/talks/amber_case_we_are_all_cyborgs_now.html">we are all cyborgs now</a>. As soon as man started using tools, we were augmenting what evolution gave us to become something more. A cyborg. When you drive a car, you are a cyborg because your legs could not carry you that fast. Welcome to the future, human.</p>

<p>That progress marches on constantly, much faster than evolution could ever provide us with enhancements. Computers, the internet, and smartphones are some of the latest and greatest in the enhancements that we add to our bodies to be something more. We&#8217;re already using smartphones to constantly be connected to a wider world than just what we can see and hear in the room. We&#8217;re extending our brains by using things like Google Search and Wikipedia to find and remember far more than our meatbrains could do on their own. And we&#8217;re connected to others to a degree that no other form of communication has matched. When scifi authors write about this kind of brain enhanced by technology, they sometimes refer to it as the exocortex. I&#8217;d argue that we already have exocortexes: our digital selves, the software and websites we use, the tools like Twitter and Facebook that we communicate over, are all part of those exocortexes. Without them, we are less than the whole. And while not everyone feels it yet, I certainly feel a little limited when cut off from my exocortex.</p>

<p>The promise, then, of Google Glass is to constantly be connected to that information source and communications tools. Not to interrupt your daily life, but to simply be better integrated into it. Some people want to point out that Glass is going to be a distraction from &#8220;real life&#8221;, but your real life already includes these &#8220;distractions&#8221; to a degree that you&#8217;re probably not thinking about. How often do you check your email during your work day? Twitter? What apps do you use on your smartphone to find the next bar to go to or figure out a restaurant that everyone can eat at? Does using any of those constitute putting real life on hold while you spend some quality time with something fake? No, they&#8217;re just part of life.</p>

<h2>The buzz</h2>

<p>I was initially quite excited about Google Glass, because it is the first real promise of wearable computing for the masses at a price point that we will probably be able to afford. For nearly a year, the product has been announced but no real details have surfaced. With the announcement of the Google Glass Explorer contest, more information has been leaking out of the Googleplex. After the <a href="http://www.youtube.com/watch?v=9c6W4CCU9M4">Youtube video showing real Glass in use</a> was posted, I quickly realized that Google Glass was not intended for me in the way that I was hoping.</p>

<p>Simply put, Glass is going to do the things that consumers now want to do quickly with their smartphones: receive and send texts &amp; email, take pictures, take videos, and post that content up to things like Facebook, Youtube, and Google+. Another valid use is quickly searching Google for something, which could be useful for everything from trivia night to trying to remember what goes in your favorite korma.</p>

<p>But the things I&#8217;m interested in? Coding while walking around (twiddling fingers in the air, no doubt), augmenting my poor social skills by having my wearable remember faces and previous conversations, providing heads-up documentation while I am doing some task, reading full ebooks in a sitting with some sort of speed-reading app, etc. And some of these, like coding and reading, are quite focused activities that would benefit from having full-screen display of information rather than just a hovering box in the corner of one eye. Since the Glass only really shows notifications and thumbnails of photos and videos, it isn&#8217;t going to provide that focused experience. At least until Google or some other company builds binocular, full-vision smart glasses.</p>

<p>Then again, building a device that most consumers will want to use is exactly what Google should be doing. I&#8217;m a relatively tiny market, and it&#8217;d make no sense for Google to prioritize my features over the average consumer. I&#8217;m well aware of that. But all that said, I still plan on being an early-adopter, at least after the price drops a little.</p>

<h2>The marketing bullshit</h2>

<p>You may have heard that <a href="http://www.independent.co.uk/news/world/americas/google-cofounder-sergey-brin-feels-emasculated-by-smartphones-8514748.html">Sergey Brin said</a> that smartphones are &#8220;emasculating.&#8221; I&#8217;m not going to linger on this topic too long, but whether he thought this up on the fly or a marketing team came up with it, it makes him sound like an idiot. And in case you forgot, Google makes smartphones. Is it &#8220;emasculating&#8221; to open a refrigerator? What about when you ride the bus rather than drive a car in to work? What does this tell female smartphone users when you say this? I believe the correct usage of the word &#8220;emasculating&#8221; is to &#8220;deprive (a man) of his male role or identity. Ex: he feels emasculated because he cannot control his sons&#8217; behavior.&#8221; What does that have to do with a smartphone? It&#8217;s just a poor choice of words all around. Sorry Sergey, but I&#8217;m calling bullshit on your statement.</p>

<h2>The wearable device Cambrian explosion</h2>

<p>We can only hope that as soon as Google Glass comes out, two things start to happen: hackers figure out how to root (or jailbreak, or whatever we&#8217;re going to call it) Glass and install Linux on it. This will ensure that open source software can start being developed for it for all sorts of niche users (like me) and not just the average consumer mentioned above. It also will help to combat some of the security and privacy concerns outlined below. Maybe in the long run, Linux isn&#8217;t the best OS to run on such a device, but that doesn&#8217;t matter initially. As soon as open source hackers figure it out, anything and everything can run on it. We&#8217;ll see a Cambrian explosion of wearable computing apps.</p>

<p>I think we&#8217;ll also see hardware manufacturers, especially the OEMs that typically make keyboards, mice, and cheap Android phones, start to produce similar glasses to the Google Glass devices. They&#8217;ll likely run Android or some flavor of whatever Microsoft is calling a mobile OS, but will probably be a lot easier to hack than the Google Glass device. And they&#8217;ll get cheaper. The best part about all of this is that by leading the way, Google is practically ensuring that we&#8217;re going to have lots of manufacturers building these devices, and they&#8217;re going to get cheaper. The demand for wearable displays in the past were limited to applications in military and some industry jobs, along with academic research, and so wearable displays tended towards the expensive and impractical in the past. Eventually, smart glasses will probably be as common as smartphones, and around the same price point. Don&#8217;t be surprised if, in a few years, it makes a lot more sense for someone to wear smart glasses than to carry around a smartphone in their pocket.</p>

<p>By being first to market, Google is also giving us a good model for what interactions with smart glasses can look like. The early, proto-smartphones like the Blackberry were arguably very poor in terms of user experience &amp; interaction design. It wasn&#8217;t until the Apple iPhone came along that we started to get good UX on mobile devices. Now, even free, low-end Android phones come with some UX heritage imprinted from Apple&#8217;s iOS. While it remains to be seen whether Glass&#8217;s UX is great and not just as good as the Youtube video makes it out to be, I hope that it helps imprint some good UX concepts onto all future devices.</p>

<h2>The &#8220;dorky&#8221; backlash</h2>

<p>I&#8217;ve seen a few blogs, perhaps in an attempt to stir up trouble and get pageviews, call out Google Glass for being dorky. You&#8217;ll see phrases like &#8220;But why would anyone want to wear something so dorky?&#8221; The authors of these pieces of trying to enforce the status quo the same way that kids in school make fun of the clothes that others wear. Why are they doing that? They may have valid fears of the technology&#8217;s privacy implications. But more likely, they&#8217;re responding to what, in literature, we&#8217;d call &#8220;the fear of the other.&#8221; The Google Glass device is new and unknown. People who would break from the tribe and wear it are weird; possibly nonhuman. They are the other.</p>

<p>There&#8217;s a fear mentioned in some blog posts about Glass that someone might pay more attention to their Glass display than to the other person in a social situation. I imagine that the way it works out in reality is that common courtesy comes into play here, and that you wouldn&#8217;t ignore someone in favor of your glass any more than you&#8217;d walk away from someone you were talking to to look at Twitter on your phone. That said, some people have done that to me, and I expect that it is really just a fault of people rather than the technology.</p>

<p>Some of this backlash, too, is the fear that someone else with that easy access to information and search results (perhaps even more discreetly than the voice searching we&#8217;ve seen so far) is going to give others an unfair advantage. Suddenly everyone will remember everything and be experts on trivia, or last quarter&#8217;s financials, or any other things that smart glasses will make easy. But again, as I discussed before, this is all part of the wearer&#8217;s exocortex. It is as much as part of them as using a hammer or using a calendar on the wall to remember something.</p>

<p>By being a part of the cyborg human, the device is a prosthetic that the wearer uses to enable their exocortex. But rather than compensating for some handicap, it helps to enhance the wearer. Would you deny someone their hearing aids because they might be able to hear more than you? I think the feelings and popular opinion on these topics will change as more people start wearing smart glasses and other wearable technology. It&#8217;s already perfectly acceptable to use your smartphone to play Angry Birds while sitting in a waiting room, and I&#8217;m pretty sure that the activities that the Google Glass will lend itself to will soon become socially acceptable, too.</p>

<h2>The fear</h2>

<p>The biggest, and most valid in my mind, issue with Google Glass is what it will mean for everyone to suddenly have an always-on, always-available camera and microphone on their face. I&#8217;d suggest you go read Mark Hurst&#8217;s article on Creative Good, <a href="http://creativegood.com/blog/the-google-glass-feature-no-one-is-talking-about/">The Google Glass feature no one is talking about</a> if you haven&#8217;t yet, to get up to speed on this debate.</p>

<p>While I believe in privacy and support organizations like the <a href="https://eff.org">EFF</a>, I think it is a little short-sighted and silly to react so strongly to the fact that the Google Glass device will have a camera on it. The two fears outlined in that article are:</p>

<ul>
<li>That anyone, at any time, could be taking photos or video of you without your consent. And you wouldn&#8217;t know.</li>
<li>That Google will now be able to index and otherwise process any audio, video, and images you send to them, along with the other data the Glass will send along: date and time, Google user account, etc.</li>
</ul>


<p>This seems to happen with every technology. And you may not realize it, but if you are advocating against the Google Glass for the above reasons, then you have far more reasons to be vocal about banning smartphones and even cheap digital cameras. I think it&#8217;s easy for someone to exclaim &#8220;But they could be taking videos of me in a public place, possibly something embarrassing!&#8221; and not realize that there already exists a whole bunch of terrible usage of existing technology out there to exploit women by taking pictures and video of them without their consent. No one seems up in arms, marching to ban the cheap digital camera on behalf of exploited women. The same technology is used by parents to take pictures and videos of their kids. Here, the technology itself is not so much to blame as the people who would use it to exploit others.</p>

<p>There have been attempts to try and regulate digital cameras and smart phones in the past, usually to force them to emit some kind of loud shutter noise when a picture is taken or a video is started. But just about everything comes with a camera now; are you really going to regulate all of that? Part of my point here is that you can&#8217;t regulate the future and try to lock down technology, because the companies that want to sell you the technology will just find a way around it. Let&#8217;s be honest here, governments are too slow to out-maneuver technology.</p>

<p>The fact of the matter is that in public, you&#8217;re probably being monitored by far more than just someone&#8217;s glasses, and that&#8217;s far more worrying than worrying about this new technology that Google is building. Security cameras abound. Your movements online are tracked by any number of ISPs, advertisement platforms, and governments. Have you been up in arms all this time about that and only now adding Google Glass to the list of technologies to worry about? I&#8217;m just pointing out the absurdity of singling out Google Glass here.</p>

<p>Now, what about the fear that, simply put, you won&#8217;t know if someone with Google Glass glasses on could be taking a video and you wouldn&#8217;t know it? Isn&#8217;t that true that they could already be doing this every time you&#8217;re standing around someone with a smartphone in their hand? The fact is that the majority of people are probably going to be polite and follow the same kinds of social expectations you&#8217;d have around that. Just like you&#8217;re already doing whenever you take out your smartphone. Do people take videos of people on the bus because they think they&#8217;re funny? Sure, but that&#8217;s not the only use for a camera on a smartphone.</p>

<p>Lastly, there&#8217;s the concern that all of the data from Glass will be going into Google, to be indexed and searched, and could be subpoenaed by the government. This problem does not really point at the technology as the source of that concern. <strong>If you&#8217;re really worried about what kinds of things a corporation or government could do with all that information, especially a corrupt government, then the problem lies with the governments and corporations.</strong> You&#8217;ve been contributing to the constant stream of information into Google and Facebook&#8217;s datacenters for years. Twitter will give up your information to the government if pressured; yet we communicate over Twitter all of the time. The government has been installing taps into data exchanges for years to monitor online communications. So think about it. It might make a lot more sense for you to focus on fixing those organizations, or weakening their growing Big Brother powers, rather than chasing after perceived rights lost when someone wears a camera on their face in public.</p>

<p>Open source, again, provides a way out here: It&#8217;s likely that an open source OS like Linux will put more control about what information it leaks into the hands of users. At the same time, the open source software probably won&#8217;t see widespread usage by the average consumer, and so we should continue to question and call out this kind of abuse of information by corporations and governments. And, we can do what Mozilla is doing with <a href="http://www.mozilla.org/en-US/firefox/partners/">Firefox OS</a> and provide software that encodes some of our ideals about freedom and privacy right into the software, while making it attractive for manufacturers to use by making it free.</p>

<p>So don&#8217;t try to shout down a fledgling technology just because it could be used to limit your freedoms or privacy. Lots of technologies <em>could</em> be used to limit those things. You can&#8217;t regulate the technology, because cheap clones are on the way and a lot of people are going to want them. Instead, the real menace here is those that would use technology to limit your privacy. Those are what you should fear when you feel uneasy about the future. I&#8217;d like to see a lot more discussion on that topic, but the latest gadget fad pays the bills (with advertising, at least) better, I suppose. And I guess I&#8217;d like to see more writing on the <strong>potential</strong> of new technologies to improve the human condition and make us better cyborgs, rather than just whether or not it will be the killer Facebook app. But we can all dream, right?</p>

<hr />

<p><a name="accelerando-note"></a></p>

<p><strong>1</strong>: In particular, <a href="static/fiction/accelerando/accelerando.html">Accelerando</a> features a main character with smart glasses. It&#8217;s an interesting portrait of how a well-connected digital savant might use this wearable technology while still interacting with people and places.
<a href="#accelerando-note-return">&#8617;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run rake. Just run rake.]]></title>
    <link href="http://blog.mattgauger.com/blog/2012/07/13/run-rake-just-run-rake/"/>
    <updated>2012-07-13T14:59:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2012/07/13/run-rake-just-run-rake</id>
    <content type="html"><![CDATA[<p>Recently, I was setting up my laptop for an existing Rails project with the help of a pair. My pair was pivot on this project, which means that he&#8217;d been on it longer and so was bringing his experience and knowledge to the table, while I was seeing the project with fresh eyes.</p>

<p>&#8220;This is going to take forever to set up,&#8221; he grumbled. &#8220;The documentation&#8217;s out of date, and I remember there&#8217;s a bunch of gotchas to setting this project up. We&#8217;ll need to compile this, then do that, then you get an API key for..&#8221;</p>

<p>I went to Github and cloned the repo to my laptop.</p>

<p>&#8220;Just run rake,&#8221; I replied.</p>

<p>&#8220;What?&#8221;</p>

<p>&#8220;Just run rake. It will tell us what to do next.&#8221;</p>

<p>And indeed, it did tell us what to do. I&#8217;ve called this test-based configuration, or other funny things in the past, but you can just think of it as trying to get to a known good state - a state where all the tests run. If it prevents the tests from running, then it&#8217;s the next thing you need.</p>

<p>I had to silence my pair&#8217;s grumbling at this process, because at first it seems like you&#8217;re going to be waiting a lot for rake, and that it might be easier just to remember all the steps necessary to set up a project.</p>

<p>It turns out rake showed us all the steps we needed to do to get the project running. A full log of what this looks like setting up a simple Rails app can be seen <a href="https://gist.github.com/3112558">in this gist</a>.</p>

<h3>Why?</h3>

<p>The point of software craftsmanship is to be pragmatic, not to seek perfection. I could have memorized the steps necessary to set up the average Rails project, but those steps wouldn&#8217;t have applied here. And indeed, my pair could have memorized them, since he had been on the project. But those steps would go out the window as soon as my pair was on another project. It is far more pragmatic to know the behavior of our tools (like knowing that rake will tell us about each thing necessary to get to a state where the tests pass) and rely on that behavior rather than to seek perfection on this one project.</p>

<p>Note: we could have used our experience with Rails and software craftsmen to avoid some of the steps you see me running in the gist: for example, you probably know that if the databases aren&#8217;t created, that you can run <code>rake db:create:all db:migrate db:test:prepare</code> all at once, without running rake inbetween every single rake task. That&#8217;d be far more pragmatic, as you&#8217;re saving yourself time and effort by knowing the toolset. But I wanted to demonstrate that running rake between every single step told us what to do next.</p>

<p>Now, ask yourself: How can you &#8220;just run rake&#8221; with your projects?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mosh, SSH Tunnels, and Tmux]]></title>
    <link href="http://blog.mattgauger.com/blog/2012/04/21/mosh-ssh-tunnels-tmux/"/>
    <updated>2012-04-21T21:14:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2012/04/21/mosh-ssh-tunnels-tmux</id>
    <content type="html"><![CDATA[<p>I&#8217;m currently preparing for RailsConf. One of the things I wanted to do before I left was figure out a way to monitor a process on a Linux server running on my LAN. The process frequently crashes and needs some prodding to restart.</p>

<p>One solution might be to set up DynDNS and configure the router to point from some random port to the SSH port on the Linux machine running on the LAN. That idea didn&#8217;t strike my fancy, as it just isn&#8217;t terribly secure to open up a port to the entire world, and it seems that DynDNS costs money to use since 2008. I&#8217;m sure there are free alternatives, but I couldn&#8217;t be bothered to find them, much less configure them.</p>

<p>A much better solution would be an SSH tunnel up to my Linode, and a reverse tunnel back down to the Linux server on my LAN. If you haven&#8217;t used reverse SSH tunneling before, it is really neat. Here&#8217;s an example:</p>

<p>From the local server:</p>

<pre><code>ssh -R 2048:localhost:22 username@example.com
</code></pre>

<p>From the server out in the Internet (aka example.com in this example)</p>

<pre><code>ssh -p 2048 other_username@localhost
</code></pre>

<p>Note that the &#8216;localhost&#8217; in the second example refers to your local server, not the one out in the internet. So <code>other_username</code> should be the username on your local server (not the remote server.) Confused yet? Good.</p>

<p>Now, this setup will get you pretty far: You&#8217;re now able to run commands on the local server from the Linode out in the cloud. But you may notice a problem fairly quickly: if you kill that SSH shell on the local server, the connection up to the server on the internet dies, and so does the reverse tunnel that was inside that.</p>

<p>Fix that by starting up a tmux session on the local server first, then detaching it.</p>

<pre><code>tmux new -s ssh-tunnel
ssh -R 2048:localhost:22 username@example.com
&lt;leader-d&gt;
</code></pre>

<p>Now the tmux session will happily keep that SSH shell open on the local server and you can reverse tunnel back over it.</p>

<p>My solution involves <a href="http://mosh.mit.edu">Mosh</a>, which I&#8217;ve been using quite a bit since it exploded on Hacker News and other news sites. Simply put, Mosh is like SSH, but it uses UDP packets to make itself more reliable. &#8220;More reliable without TCP?&#8221; you say? Well, Mosh is doing a little more work to buffer the connection to the other machine (including instant response to typing when there&#8217;d otherwise be lag) and maintains that connection: even if you change IPs, hop on a train, etc. It&#8217;s pretty amazing, and so far I&#8217;ve been loving it. Click through to the Mosh site to read up more on it. It really is awesome.</p>

<p>With Mosh, we add another layer to the puzzle, so that my final setup looked like this:</p>

<pre><code>matts_laptop$ mosh matt@someserver.local
matt@someserver.local$ tmux new -s ssh-tunnel
&lt;tmux session starts&gt;
matt@someserver.local$ ssh -R 2048:localhost:22 username@example.com
&lt;leader-d&gt;
</code></pre>

<p>Pop open another window in my local tmux session (are you beginning to see a pattern?) and then connect with mosh up to my Linode:</p>

<pre><code>matts_laptop$ mosh me@mattgauger.com
me@mattgauger.com$ tmux new -s server-monitor
&lt;tmux session starts&gt;
me@mattgauger.com$ ssh -p 2048 matt@localhost # Note: this is the matt@someserver.local account!
matt@someserver.local$ &lt;monitor the process I'm concerned with&gt;
</code></pre>

<p>Now I&#8217;ve got this mostly-persistent (thanks to mosh and tmux) session that I can detach from if I really need to, but I&#8217;ll still be able to connect back up to my Linode and check on the server on my LAN from RailsConf.</p>

<p>Pretty cool, huh?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Announcing a virtual book club: The Diamond Age]]></title>
    <link href="http://blog.mattgauger.com/blog/2012/04/13/announcing-a-new-virtual-book-club-the-diamond-age/"/>
    <updated>2012-04-13T15:29:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2012/04/13/announcing-a-new-virtual-book-club-the-diamond-age</id>
    <content type="html"><![CDATA[<p><img src="http://blog.mattgauger.com/images/The_Diamond_Age.jpg" alt="The Diamond Age book cover" /></p>

<p>Recently I decided that it&#8217;s time to re-read The Diamond Age by Neal Stephenson. But, without external motivators, the list of books that I&#8217;d like to read grows and grows, and reading books actual takes me a lot longer than it should. (Because, well, the internet is so damn interesting, and I haven&#8217;t finished reading that.)</p>

<p>So I&#8217;m trying an experiment. I am proposing to hold a book club, online, so that I will read The Diamond Age again. My current plan is to use Google Plus Hangouts and hold weekly online meetings to discuss progress in the book. I&#8217;ve never really run a book club on my own before, so we&#8217;ll see where this goes.</p>

<p>If you&#8217;re interested, please indicate that interest by signing up below. I&#8217;ll use the sign ups to plan and gauge when to start.</p>

<h3><a href="https://docs.google.com/spreadsheet/viewform?formkey=dFBBNUJZcUZuX1Q3Uk5VbGRVRVNsQ3c6MQ#gid=0">Sign up form</a></h3>

<p></p>


<p>Get The Diamond Age: (non-affiliate links)</p>

<ul>
<li><a href="http://www.amazon.com/The-Diamond-Age-Illustrated-Spectra/dp/0553380966/">Paperback</a></li>
<li><a href="http://www.amazon.com/Diamond-Bantam-Spectra-Book-ebook/dp/B000FBJCKI/">Kindle Edition</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New site on Octopress and Twitter Bootstrap]]></title>
    <link href="http://blog.mattgauger.com/blog/2012/03/25/new-site-on-octopress-and-twitter-bootstrap/"/>
    <updated>2012-03-25T13:52:00-05:00</updated>
    <id>http://blog.mattgauger.com/blog/2012/03/25/new-site-on-octopress-and-twitter-bootstrap</id>
    <content type="html"><![CDATA[<p>Hi there. As you may have noticed, my blog is moving over to Github pages. I&#8217;ve used Octopress as a starting point, and ported a <a href="https://github.com/jlong/sass-twitter-bootstrap">SASS version of Twitter Bootstrap</a> to use for building the layout.</p>

<p>Some people called out the fact that I&#8217;m using Twitter Bootstrap on Twitter. When Twitter Bootstrap first came out, I was appalled to see so many sites turning into Twitter through Bootstrap, but I&#8217;m over it now. Bootstrap provides a GREAT grid system and makes it very fast to get a responsive layout up. Plus, I liked my <a href="https://twitter.com/mathiasx">Twitter profile background</a> so much that I made this site look like it. So just deal with it.</p>

<p>I still have a bit of work to do to copy over my blog posts from Posterous. Jekyll provides an exporter script that I&#8217;ve tried, but it resulted in a lot of data loss. I have to decide whether I&#8217;m going to finish writing <a href="https://github.com/mathias/posterous-exporter">my own script</a> or just export the blog posts manually, since they require a lot of HTML cleanup. (I used the Posterous WYSIWYG editor to write quite a few of those blog posts, which resulted in a ton of inline styles and weird paragraph / block elements being inserted.)</p>

<p>You can subscribe to the Atom feed above to receive updates in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MadJS February 2012 - How CoffeeScript & Jasmine Made Me a Better JavaScript Developer]]></title>
    <link href="http://blog.mattgauger.com/blog/2012/02/17/madjs-february-2012-how-coffeescript-jasmine-made-me-a-better-javascript-developer/"/>
    <updated>2012-02-17T00:00:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2012/02/17/madjs-february-2012-how-coffeescript-jasmine-made-me-a-better-javascript-developer</id>
    <content type="html"><![CDATA[<p>Recently, I spoke at the MadJS group about CoffeeScript &amp; Jasmine. My slides appear below. Since Slideshare doesn&#8217;t show my notes, you can either download the keynote file above, or you can read the notes below, which I&#8217;ve pulled out of the slides. I feel that the notes are necessary to know what I&#8217;m talking about, and I hate reading slide decks where there&#8217;s no context or notes. This isn&#8217;t going to be as helpful as having seen my talk in person, but hopefully you get some value out of my talk and the notes together:</p>
<div style=""><strong style="display: block; margin: 12px 0 4px;"><a href="http://www.slideshare.net/mathiasx/coffeescript-jasmine-madjs-february-2012" title="CoffeeScript &amp; Jasmine - MadJS February 2012" target="_blank">CoffeeScript &amp; Jasmine - MadJS February 2012</a></strong> <iframe marginheight="0" scrolling="no" src="http://www.slideshare.net/slideshow/embed_code/11637009" marginwidth="0" frameborder="0" height="355" width="425"></iframe>
<div style="padding: 5px 0 12px;">View more <a href="http://www.slideshare.net/" target="_blank">presentations</a> from <a href="http://www.slideshare.net/mathiasx" target="_blank">Matt Gauger</a></div>
</div>
<h2>How CoffeeScript &amp; Jasmine made me a better JS developer</h2>
<h3>First, an introduction:</h3>
<ul>
<li>I&rsquo;m Matt Gauger</li>
<li>@mathiasx on twitter</li>
</ul>
<h3>I work at <a href="http://bendyworks.com">Bendyworks</a></h3>
<ul>
<li>We primarily do Ruby on Rails work, with iOS now.</li>
<li>We care very deeply about software craftsmanship and honing our agile practices.</li>
</ul>
<h3>Which leads me to my dilemma</h3>
<ul>
<li>Are you familiar with impostor syndrome?</li>
<li>It&#8217;s the idea that even very skilled practitioners may sometimes feel like an impostor due to over-emphasizing their weaknesses.</li>
<li>Further, it&rsquo;s the inability to internalize your own accomplishments.</li>
</ul>
<h3>I felt like an impostor when it came to JavaScript.</h3>
<ul>
<li>Of course, I could read and write the syntax, pull in jQuery, manipulate the DOM, etc.</li>
<li>I had several projects under my belt at this time that used AJAX and were fairly complex</li>
<li>I&#8217;d even read JavaScript: The Good Parts several times, taken notes, etc.</li>
</ul>
<h3>So, what does this have to do with CoffeeScript?</h3>
<ul>
<li>My thesis: How CoffeeScript &amp; Jasmine made me a better JS developer (and how it can help you, too)</li>
<li>But before that: Let me warn you that I&#8217;m not going to go over every piece of syntax here.</li>
<li>I&#8217;m not going to be able to teach you all of CoffeeScript or Jasmine in this talk.</li>
<li>For that, see the resources &amp; books at the end of the talk.</li>
</ul>
<h3>CoffeeScript History</h3>
<ul>
<li>2010 I learn about CoffeeScript, and it sort of looks like Ruby and Python.</li>
<li>It grabs my interest.</li>
<li>But at that point it&#8217;s still a novelty: The compiler is in Ruby, no one uses it for real dev yet.</li>
<li>It was a *toy*</li>
</ul>
<h3>Today</h3>
<ul>
<li>Flash-forward to today, and everyone is extolling CoffeeScript. </li>
<li>It comes with Rails 3 by default now and gets compiled on the fly into JS for your app.</li>
<li>I&rsquo;ve been using CoffeeScript for about a year now.</li>
</ul>
<h3>CoffeeScript has Good Parts?</h3>
<p>So why use CoffeeScript? What are its good parts?</p>
<ul>
<li>It restricts you to a subset of JS you&rsquo;d recognize from JS: The Good Parts.</li>
<li>It puts JSLint &amp; a compiler between me and the half dozen browsers I need to support</li>
<li>It warns me when I do something wrong</li>
</ul>
<p>This might be the most important part of the talk, and reason to use CoffeeScript</p>
<ul>
<li>If you&rsquo;re like me, you&rsquo;ll put the compiled JS up next to the CoffeeScript</li>
<li>By reading the output of the compiler, you&rsquo;re learning what good JS looks like.</li>
</ul>
<h3>Criticisms of CoffeeScript:</h3>
<ul>
<li>It&#8217;s not what runs in the browser.</li>
<li>Difficult to debug =&gt; Finding bugs (Names are the same between CoffeeScript &amp; JS; its readable)</li>
<li>May feel like you&rsquo;re learning a whole different language (It&rsquo;s not, it&rsquo;s less verbose JS)</li>
</ul>
<h3>That isn&#8217;t to say that CoffeeScript eliminates all bugs</h3>
<ul>
<li>At this point we may want to differentiate between bugs that are caused by poor syntax and mistakes (mistake bugs), and bugs that come from the interaction between complicated data &amp; edge cases (ie computer is in a state you didn&#8217;t predict when you wrote the code)</li>
<li>CoffeeScript can help cut down on a lot of the former.</li>
</ul>
<h3>Some examples of CoffeeScript helping you with bugs:</h3>
<p>Coercing the wrong types</p>
<ul>
<li>This will print it&rsquo;s true happily.</li>
<li>That&rsquo;s not quite what you expect when the data is more complicated than 1 and the string 1.</li>
<li>Type coercion is the number 1 reason for <a href="http://wtfjs.com/">WTFJS</a></li>
<li>Ok, so that&rsquo;s a very simple example.</li>
<li>But how often are you going to get bitten by more complicated versions of that same bug?</li>
<li>And are you always going to remember to use triple equals? === I am now.</li>
</ul>
<h3>Scope</h3>
<ul>
<li>CoffeeScript scope is essentially the same as in JS</li>
<li>JS and CoffeeScript have &ldquo;lexical scope&rdquo;</li>
<li>1. <span> </span>Every function creates a scope, and the only way to create a scope is to define a function.</li>
<li>2. A variable lives in the outermost scope in which an assignment has been made to that variable.</li>
<li>3.<span> </span> Outside of its scope, a variable is invisible.</li>
<li>The neat thing is that CoffeeScript&rsquo;s compiler places the vars for each scope at the top of that scope</li>
<li>Define a variable at a specific scope by giving it a sensible initial value</li>
<li>Hopefully this is better than &lsquo;null&rsquo;, but you could do worse and just not initialize it at all.</li>
<li>JavaScript won&rsquo;t force you to initialize it, but doing so can help you to figure out scope issues.</li>
</ul>
<h3>a ?= b</h3>
<ul>
<li>the ?= is syntactic sugar, the ? is called the existential operator in CoffeeScript</li>
<li>Combined with =, the existential operator means &ldquo;a equals b unless a?&rdquo; or     
<ul>
<li>&ldquo;Let b be the default value for a.&rdquo;</li>
</ul>
</li>
</ul>
<h3>Lastly, wrapping up your code.</h3>
<ul>
<li>CoffeeScript can wrap each compiled file into a scope</li>
<li>This may be the default, depending on the version of coffeescript you&rsquo;re using - you might need to pass an option now to either wrap or not wrap your code in a scope.</li>
<li>This is actually pretty cool &#8211; if you&rsquo;re including a lot of JavaScripts on a website, you can&rsquo;t mix scope there &#8211; no accidental leakage into the global scope space.</li>
<li>Compiled CoffeeScript Example: </li>
</ul>
<div class="CodeRay">
  <div class="code"><pre>(function() {
  console.log &quot;hello world!&quot;;
}).call(this);</pre></div>
</div>

<h3>Simpler Looping</h3>
<ul>
<li>You write list comprehensions rather than for loops in CoffeeScript</li>
<li>Comprehensions are expressions, and can be returned and assigned</li>
</ul>
<h3>Jeremy Ashkenas&rsquo;s Example</h3>
<ul>
<li>Loop over every item in a list, in CoffeeScript:</li>
</ul>
<div class="CodeRay">
  <div class="code"><pre>for item in list
  process item</pre></div>
</div>

<h3>Intention gets obscured by managing the loop, in JS:</h3>
<div class="CodeRay">
  <div class="code"><pre>for (var i = 0, l = list.length; i &lt; l; i++) {
  var item = list[i];
  process(item);
}</pre></div>
</div>

<li>CoffeeScript allows &ldquo;reasonably solid&rdquo; JavaScript developers to accomplish the latter by simply writing the former.</li>
<h3>In Review:&nbsp;CoffeeScript will help you:</h3>
<ul>
<li>Write OO, Prototype-based code</li>
<li>Avoid bugs in comparisons</li>
<li>Stop using ==, only use ===</li>
<li>Manage scope and avoid state through scope creep</li>
<li>Reduce off-by-one errors in looping, and generally write better loops than you were writing before</li>
</ul>
<h3>Jasmine</h3>
<p>(My) History (with Jasmine)</p>
<ul>
<li>I started using Jasmine last summer on a client project.</li>
<li>It&rsquo;s enough like the BDD tool we use in Rails, Cucumber, that I consider it a BDD tool.</li>
<li>It makes the most sense to me of the BDD/TDD tools in JS I&rsquo;ve used</li>
</ul>
<h3>Why Jasmine?</h3>
<ul>
<li>All code should be tested =&gt; that&rsquo;s what I believe.</li>
<li>You can spend some up-front time testing your code, or you can spend a lot of time bug fixing later</li>
<li>I realize that not all legacy codebases are going to have full test coverage overnight.</li>
</ul>
<h3>The example on the Jasmine site:</h3>
<div class="CodeRay">
  <div class="code"><pre>describe(&quot;Jasmine&quot;, function() {
  it(&quot;makes testing awesome!&quot;, function() {
    expect(yourCode).toBeLotsBetter();
  });
});</pre></div>
</div>

<ul>
<li>This example sucks!</li>
<li>A better example:</li>
</ul>
<div class="CodeRay">
  <div class="code"><pre>describe ('addition', function() {
  it('adds two numbers', function() {
    expect(1 + 2).toEqual(3);
  });
});</pre></div>
</div>

<ul>
<li>Better? Not really. But we can see what the syntax is doing here and I&rsquo;m using a real assertion!</li>
</ul>
<h3>How should we test JS?</h3>
<ul>
<li>Functions should not depend on the DOM</li>
<li>Our Logic needs to be in separate pieces     
<ul>
<li>to make it easier to test the logic, things like AJAX calls, etc</li>
<li>without interacting with the DOM</li>
</ul>
</li>
</ul>
<h3>Easier to test = better code</h3>
<ul>
<li>it just so turns out, that the abstraction for testing is a better abstraction overall</li>
<li>I&#8217;ve heard &#8220;The first implementation of your code is the unit tests&#8221; so it may not be DRY, but tests should show how to implement your code!</li>
</ul>
<h3>Follow TDD/BDD:</h3>
<h3>red, green, refactor</h3>
<ul>
<li>You can still do this in Jasmine, in fact, I find it kind of natural.</li>
</ul>
<h3>Jasmine is designed to be standalone</h3>
<ul>
<li>This means you don&rsquo;t need jQuery and you don&rsquo;t need to run it in a real browser (but you can)</li>
</ul>
<h3>Some really cool features of Jasmine:</h3>
<p>Matchers:</p>
<ul>
<li>.toBe()</li>
<li>.toBeNull()</li>
<li>.toBeTruthy()</li>
<li>.toBeDefined()</li>
<li>.toBeUndefined()</li>
</ul>
<h3>Setup and teardown:</h3>
<div class="CodeRay">
  <div class="code"><pre>beforeEach()
afterEach()</pre></div>
</div>

<ul>
<li class="li1">You can use after Each to run a teardown function after each successful test</li>
<li class="li1">If you need a teardown function after a test whether it passed or failed, use after()</li>
</ul>
<h3>Spies: built-in mocking &amp; stubbing</h3>
<ul>
<li>In Ruby, we&rsquo;ve been doing mocking and stubbing for awhile.</li>
<li>Jasmine&rsquo;s spies make it easy!</li>
<li>These let you do things like watch to see if a method was called</li>
<li>Or to stub out other methods so you don&rsquo;t do real AJAX calls, etc.</li>
</ul>
<h3>But what about legacy codebases?</h3>
<ul>
<li>So you&rsquo;re thinking, &#8220;CoffeeScript and Jasmine sound great, but I have a legacy codebase.&#8221;</li>
<li>Or, &#8220;I&rsquo;ll never get to use either; and they don&rsquo;t help my big legacy codebase.&#8221;</li>
<li>Well, we&rsquo;ve run into this and I have a plan.</li>
</ul>
<h3>Start simple.</h3>
<ul>
<li>First, get your tools lined up.</li>
<li>Get the CoffeeScript compiler in your tool chain</li>
<li>Get Jasmine set up and passing a dummy test.</li>
<li>You still haven&rsquo;t done anything with your legacy code at this point.</li>
</ul>
<h3>Fix one bug.<br />(red, green, refactor)</h3>
<ul>
<li>It all starts with one bug. Or one feature, if you&rsquo;re feeling adventurous.&nbsp;</li>
<li>You might not be able to pull out an entire feature and rewrite it. I understand that. Don&#8217;t give in to this temptation yet!</li>
<li>The way to start this is to write a test around the bug and see it fail. (this might be hard -&gt; depending on how tied your code is to the DOM &#8211; see Jasmine-JQuery for DOM Fixtures)</li>
<li>Then fix the bug in regular old JavaScript. See the Jasmine test pass.</li>
</ul>
<h3>Rewrite the affected code in CoffeeScript.</h3>
<ul>
You&rsquo;ve got a working test around this bug.
<li>(You know the test works because you saw it red then green.)</li>
<li>Now&rsquo;s your chance to rewrite it in CoffeeScript. It may only be one function at this point. That&rsquo;s ok.</li>
</ul>
<h3>Start grouping in files / modules.</h3>
<ul>
<li>We found that even with a legacy codebase of a lot of JavaScript, we were able to figure out logical chunks that should live together in CoffeeScript files.</li>
</ul>
<h3>Keep improving the codebase.</h3>
<ul>
<li>This is the hardest part.</li>
<li>The temptation is there to just give up and fix bugs only in JS, not to write unit tests, etc.</li>
<li>The other temptation is the one you usually can&rsquo;t give into, which is to try to rewrite everything all at once -&gt; this rarely is accomplishable, it&rsquo;s better to stage the changes.</li>
<li>The big rewrite doesn&#8217;t work!</li>
</ul>
<h3>Lessons Learned:</h3>
<ul>
<li>&nbsp;These tools can help you learn JS better.</li>
<li>Legacy codebases can slowly grow better through using CoffeeScript &amp; Jasmine.</li>
<li>Taking advantage of these is up to you! </li>
</ul>
<h3>Thanks!</h3>
<h3>Resources to learn more:</h3>
<ul>
<li><span class="s2"><a href="http://jashkenas.github.com/coffee-script/">http://jashkenas.github.com/coffee-script/</a></span></li>
<li><span class="s2"><a href="http://pivotal.github.com/jasmine/">http://pivotal.github.com/jasmine/</a></span></li>
<li><span class="s2"><a href="http://pragprog.com/book/tbcoffee/coffeescript">http://pragprog.com/book/tbcoffee/coffeescript</a></span></li>
<li><a href="http://js2coffee.org"><span class="s2">http://js2coffee.org</span></a>/</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Invalid gemspec in [.rvm/gems/ruby-1.9.2-p180@gemset/specifications/actionmailer-3.2.0.gemspec]: Illformed requirement ["# 3.2.0"] ]]></title>
    <link href="http://blog.mattgauger.com/blog/2012/01/21/invalid-gemspec-in-rvm-gems-ruby-1-9-2-p180-gemset-specifications-actionmailer-3-2-0-gemspec-illformed-requirement-3-2-0-/"/>
    <updated>2012-01-21T00:00:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2012/01/21/invalid-gemspec-in-rvm-gems-ruby-1-9-2-p180-gemset-specifications-actionmailer-3-2-0-gemspec-illformed-requirement-3-2-0-</id>
    <content type="html"><![CDATA[<p>Recently while trying to create a new Rails 3.2 project, I ran into this error after creating a new RVM gemset in Ruby 1.9.2-p180 and a Gemfile requiring only Rails 3.2.0:</p>
<div class="CodeRay">
  <div class="code"><pre>$ bundle
Fetching source index for http://rubygems.org/
Installing rake (0.9.2.2) 
Installing i18n (0.6.0) 
Installing multi_json (1.0.4) 
Installing activesupport (3.2.0) 
Installing builder (3.0.0) 
Installing activemodel (3.2.0)
Invalid gemspec in [/Users/mathiasx/Developer/.rvm/gems/ruby-1.9.2-p180@big_fan/specifications/activemodel-3.2.0.gemspec]: Illformed requirement [&quot;# 3.2.0&quot;]
... These Illformed requirement errors continue for every package Rails wants ...</pre></div>
</div>

<p>I thought that I might be able to continue on and ignore these errors, but I hadn&#8217;t seen anything like them anymore.</p>
<div class="CodeRay">
  <div class="code"><pre>$ rails new .
Invalid gemspec in [/Users/mathiasx/Developer/.rvm/gems/ruby-1.9.2-p180@big_fan/specifications/actionmailer-3.2.0.gemspec]: Illformed requirement [&quot;# 3.2.0&quot;]
... again, it throws this error many times ...</pre></div>
</div>

<p>That didn&#8217;t generate a new Rails project in my current directory, so something was clearly wrong. But what does <strong>Illformed request: Syck:DefaultKey</strong> mean? Well, it turns out that the gemspecs of requirements in Rails 3.2.0 are using a new format that older Rubygems can&#8217;t parse. The first indicator was that my version of Rubygems was out of date:</p>
<div class="CodeRay">
  <div class="code"><pre>$ gem -v
Invalid gemspec in [/Users/mathiasx/Developer/.rvm/gems/ruby-1.9.2-p180@big_fan/specifications/actionmailer-3.2.0.gemspec]: Illformed requirement [&quot;# 3.2.0&quot;]
... I've cut out a bunch of the output from the invalid gemspecs here ...
1.8.8</pre></div>
</div>

<p>We&#8217;d like to be on Rubygems 1.8.13 or newer, but I also don&#8217;t want to see those invalid gemspec warnings anymore, so I clear out my gemset with:</p>
<div class="CodeRay">
  <div class="code"><pre>$ rvm gemset empty
WARN: Are you SURE you wish to remove the installed gemset for gemset 'ruby-1.9.2-p180@big_fan' (/Users/mathiasx/Developer/.rvm/gems/ruby-1.9.2-p180@big_fan)?
(anything other than 'yes' will cancel) &gt; yes
$ cd ..
$ cd project/
Using /Users/mathiasx/Developer/.rvm/gems/ruby-1.9.2-p180 with gemset big_fan</pre></div>
</div>

<p>Note that I have my rvmrc file like this so that it created and trusted the gemset upon encountering it:</p>
<div class="CodeRay">
  <div class="code"><pre>$ cat .rvmrc
rvm use 1.9.2@big_fan --create</pre></div>
</div>

<h2>Getting everything working again:</h2>
<p>Upgrade your Rubygems (this will only apply to this version of Ruby in RVM, not all versions of Ruby)</p>
<div class="CodeRay">
  <div class="code"><pre>$ gem update --system
== 1.8.15 / 2012-01-06

* 1 bug fix:

  * Don't eager load yaml, it creates a bad loop. Fixes #256


------------------------------------------------------------------------------

RubyGems installed the following executables:
        /Users/mathiasx/Developer/.rvm/rubies/ruby-1.9.2-p180/bin/gem

RubyGems system software updated</pre></div>
</div>

<p>Then just to be safe, make sure the gems we have are pristine (According to the manpage, gem pristine: &#8220;Restores installed gems to pristine condition from files located in the gem cache.&#8221;)</p>
<div class="CodeRay">
  <div class="code"><pre>$ gem pristine --all</pre></div>
</div>

<p>And it is safe to now bundle:</p>
<div class="CodeRay">
  <div class="code"><pre>$ bundle
Fetching source index for http://rubygems.org/
Installing rake (0.9.2.2) 
Installing i18n (0.6.0) 
Installing multi_json (1.0.4) 
Installing activesupport (3.2.0) 
Installing builder (3.0.0) 
Installing activemodel (3.2.0) 
Installing erubis (2.7.0) 
Installing journey (1.0.0) 
Installing rack (1.4.0) 
Installing rack-cache (1.1) 
Installing rack-test (0.6.1) 
Installing hike (1.2.1) 
Installing tilt (1.3.3) 
Installing sprockets (2.1.2) 
Installing actionpack (3.2.0) 
Installing mime-types (1.17.2) 
Installing polyglot (0.3.3) 
Installing treetop (1.4.10) 
Installing mail (2.4.1) 
Installing actionmailer (3.2.0) 
Installing arel (3.0.0) 
Installing tzinfo (0.3.31) 
Installing activerecord (3.2.0) 
Installing activeresource (3.2.0) 
Using bundler (1.0.18) 
Installing json (1.6.5) with native extensions 
Installing rack-ssl (1.3.2) 
Installing rdoc (3.12) 
Installing thor (0.14.6) 
Installing railties (3.2.0) 
Installing rails (3.2.0) 
Your bundle is complete! Use `bundle show [gemname]` to see where a bundled gem is installed.</pre></div>
</div>

<p>Now we should have a happy gemset and the error will be gone. Let me know if you have any questions. Happy hacking!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Getting MongoDB and Devise to play well on Rails 3]]></title>
    <link href="http://blog.mattgauger.com/blog/2011/01/03/getting-mongodb-and-devise-to-play-well-on-rails-3/"/>
    <updated>2011-01-03T00:00:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2011/01/03/getting-mongodb-and-devise-to-play-well-on-rails-3</id>
    <content type="html"><![CDATA[<p>While there&#8217;s a <a href="http://www.mongodb.org/display/DOCS/Rails+3+-+Getting+Started">good guide</a> on the MongoDB site about getting mongo_mapper to work in Rails 3, I ran into some additional issues getting the popular <a href="https://github.com/plataformatec/devise">devise</a>&nbsp;<span style="font-family: helvetica, arial, freesans, clean, sans-serif; line-height: 20px;">authentication engine for Rails to work with Mongo. This documents how to create a Rails app from scratch that uses both MongoDB and Devise. So if you don&#8217;t want to reinvent the wheel on authentication (read: Users, login, logout, etc) and want to run your app on MongoDB, this should be useful.</span></p>
<p><span style="font-family: helvetica, arial, freesans, clean, sans-serif; line-height: 20px;">First of all, you&#8217;ll need Rails 3. I&#8217;m on Rails 3.0.3. I created a new gemset for this app, just to keep things clean. Run the rails new command with the&nbsp;</span><span style="font-family: Courier New, Courier, monospace; line-height: 16px;">&#8211;skip-active-record </span>switch.</p>
<div class="CodeRay">
  <div class="code"><pre>$ rails new awesome_app --skip-active-record</pre></div>
</div>

<p>Open up the Gemfile of the new app. It&#8217;s going to be pretty empty to start. This is what I ended up with, after reading the Mongo guide mentioned at the beginning of the post:</p>
<div class="CodeRay">
  <div class="code"><pre>require 'rubygems'
require 'mongo'

source :rubygems

gem 'mongo_mapper'
gem 'rails', '3.0.3'
gem 'devise', '1.1.3'
gem 'devise-mongo_mapper',
  :git    =&gt; 'git://github.com/collectiveidea/devise-mongo_mapper'

group :test, :development do
  [whatever testing gems you want in here]
end</pre></div>
</div>

<p>You&#8217;ll notice the devise-mongo_mapper gem in there. That&#8217;s the secret sauce that lets us use mongo_mapper as the ORM for Devise. As I haven&#8217;t really played with the mongoid gem (and therefore don&#8217;t have any experience with it) I didn&#8217;t try to get mongoid to work.</p>
<p>Go ahead and run a bundle install:</p>
<div class="CodeRay">
  <div class="code"><pre>$ bundle install</pre></div>
</div>

<p>Then run this to install devise files into the Rails app:</p>
<div class="CodeRay">
  <div class="code"><pre>$ rails generate devise:install</pre></div>
</div>

<p>There&#8217;s two initializer files we&#8217;ll need. We add the one for mongo, which I put in config/initializers/mongo.rb:</p>
<div class="CodeRay">
  <div class="code"><pre>MongoMapper.connection = Mongo::Connection.new('localhost', 27017)
MongoMapper.database = &quot;awesome-app-#{Rails.env}&quot;

if defined?(PhusionPassenger)
   PhusionPassenger.on_event(:starting_worker_process) do |forked|
     MongoMapper.connection.connect_to_master if forked
   end
end</pre></div>
</div>

<p>And one for devise, which you&#8217;ll find created for you in config/initializers/devise.rb. Change the ORM Configuration settings to this:</p>
<div class="CodeRay">
  <div class="code"><pre># ==&gt;; ORM configuration
# Load and configure the ORM. Supports :active_record (default) and
# :mongoid (bson_ext recommended) by default. Other ORMs may be
# available as additional gems.
  require 'devise/orm/mongo_mapper'</pre></div>
</div>

<p>Be sure to add this line to config/application.rb and edit in the appropriate address. This will keep Devise from complaining later:</p>
<div class="CodeRay">
  <div class="code"><pre>config.action_mailer.default_url_options = { :host =&gt; &quot;yourdomain.com&quot; }</pre></div>
</div>

<p>The last step is to create an User model and tell Devise and mongo_mapper to do their thing. Tell Devise to make a Users model and then install the Devise Views to our app so that we can modify them later, if we wish:</p>
<div class="CodeRay">
  <div class="code"><pre>$ rails generate devise users
$ rails generate devise:views</pre></div>
</div>

<p>In app/models/user.rb:</p>
<div class="CodeRay">
  <div class="code"><pre>class User
  include MongoMapper::Document         
  plugin MongoMapper::Devise

  devise :database_authenticatable, :confirmable, :lockable, 
         :recoverable, :rememberable, :registerable, :trackable, 
         :timeoutable, :validatable, :token_authenticatable

  attr_accessible :email, :password, :password_confirmation
   
end</pre></div>
</div>

<p>You can, of course, choose which of those Devise options to enable for your user model. Refer to the <a href="http://rubydoc.info/github/plataformatec/devise/master/Devise/Models">devise documentation</a> for more information.</p>
<p>In app/controllers/application_controller.rb:</p>
<div class="CodeRay">
  <div class="code"><pre>class ApplicationController &lt; ActionController::Base
  protect_from_forgery
  
  filter_parameter_logging :password, :password_confirmation
  
  def after_sign_out_path_for(resource_or_scope)
    new_user_session_path
  end
end</pre></div>
</div>

<p>In config/routes.rb add these lines:</p>
<div class="CodeRay">
  <div class="code"><pre>devise_for :users, :admin
resource :user</pre></div>
</div>

<p>At this point, you should probably check your app by running a quick `rails server` and seeing it if spits out any errors to your terminal. If it&#8217;s all good, then you are probably thinking you&#8217;ll want to actually use this authentication system now. Let&#8217;s add a very basic &#8220;Home&#8221; controller:</p>
<div class="CodeRay">
  <div class="code"><pre>$ rails generate controller Home index token</pre></div>
</div>

<p>In app/controllers/home_controller.rb, add the following before_filter line to the beginning of the class so that it looks like this:</p>
<div class="CodeRay">
  <div class="code"><pre>class HomeController &lt; ApplicationController
  before_filter :authenticate_user!, :only =&gt; :token
  
  def index
  end

  def token
  end

end</pre></div>
</div>

<p>In app/views/home/index.haml (I&#8217;m using HAML, but I&#8217;ve also included an ERb example after this:</p>
<div class="CodeRay">
  <div class="code"><pre>- if user_signed_in?
  %ul
    %li= current_user.email
    %li= link_to 'My info', edit_user_path
    %li= link_to 'Sign out', destroy_user_session_path
- else
  %ul
    %li= link_to 'Sign in', new_user_session_path
    %li= link_to 'Sign up', new_user_path</pre></div>
</div>

<div class="CodeRay">
  <div class="code"><pre># ERb version of app/views/home/index.html.erb:
&lt;% if user_signed_in? -%&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;%= current_user.email %&gt;&lt;/li&gt;
    &lt;li&gt; link_to 'My info', edit_user_registration_path %&gt;&lt;/li&gt;
    &lt;li&gt;&lt;%= link_to 'Sign out', destroy_user_session_path %&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;% else -%&gt;
  &lt;/code&gt;&lt;ul&gt;&lt;code&gt;
    &lt;li&gt;&lt;%= link_to 'Sign in', new_user_session_path %&gt;&lt;/li&gt;
    &lt;li&gt;&lt;%= link_to 'Sign up', new_user_path %&gt;&lt;/li&gt;
&lt;% end -%&gt;</pre></div>
</div>

<p>This will enable very basic login / logouts using the Devise views that we installed earlier. If you run rails server again, you&#8217;ll be able to create an account. But, if your system isn&#8217;t set up to send mail (like mine) then you may get an error, or simply won&#8217;t get a confirmation code, so you won&#8217;t be able to login with that user. Here&#8217;s a quick solution. Drop into the mongo shell:</p>
<div class="CodeRay">
  <div class="code"><pre>$ mongo
MongoDB shell version: 1.6.4</pre></div>
</div>

<p>Use your database, which you set above in config/initializers/mongo.rb. In this case, it&#8217;s awesome-app-development:</p>
<div class="CodeRay">
  <div class="code"><pre>&gt; use awesome-app-development
switched to db awesome-app-development</pre></div>
</div>

<p>Find all the entries in the Users document:</p>
<div class="CodeRay">
  <div class="code"><pre>&gt; db.users.find();
{ &quot;_id&quot; : ObjectId(&quot;4d216ae217cacc289c000005&quot;), &quot;email&quot; : &quot;matt.gauger@gmail.com&quot;, &quot;encrypted_password&quot; : &quot;$2aasdf&quot;, &quot;password_salt&quot; : &quot;$2aasdf&quot;, &quot;authentication_token&quot; : null, &quot;remember_token&quot; : null, &quot;remember_created_at&quot; : null, &quot;reset_password_token&quot; : null, &quot;confirmation_token&quot; : &quot;YsFg8CFBwNIm5kof7xC9&quot;, &quot;confirmed_at&quot; : null, &quot;confirmation_sent_at&quot; : &quot;Mon Jan 03 2011 00:21:22 GMT-0600 (CST)&quot;, &quot;failed_attempts&quot; : 0, &quot;unlock_token&quot; : null, &quot;locked_at&quot; : null, &quot;sign_in_count&quot; : 0, &quot;current_sign_in_at&quot; : null, &quot;last_sign_in_at&quot; : null, &quot;current_sign_in_ip&quot; : null, &quot;last_sign_in_ip&quot; : null }</pre></div>
</div>

<p>The bit we need is the confirmation_token: &#8220;YsFg8CFBwNIm5kof7xC9&#8221;. Copy the token and go to the following in your browser:</p>
<div class="CodeRay">
  <div class="code"><pre>http://localhost:3000/users/confirmation?confirmation_token=YsFg8CFBwNIm5kof7xC9</pre></div>
</div>

<p>Remember to replace your token with the one in that URL. You could alternatively make it so your app can send mail, or just turn off :confirmable in your Users model. This was a quick little solution that I found and wanted to share.</p>
<p>&nbsp;</p>
<p>Hopefully this gets you going with Mongo and Devise quickly and without any snags.&nbsp;There&#8217;s a lot more to Devise, so I&#8217;d recommend you start looking at some of the <a href="https://github.com/plataformatec/devise/wiki/Example-Applications">Example applications</a>&nbsp;and the <a href="https://github.com/plataformatec/devise/wiki">documentation</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What's new and great in Rails 3 - Milwaukee Ruby Users Group - December 2010]]></title>
    <link href="http://blog.mattgauger.com/blog/2010/12/31/what-s-new-and-great-in-rails-3-milwaukee-ruby-users-group-december-2010/"/>
    <updated>2010-12-31T00:00:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2010/12/31/what-s-new-and-great-in-rails-3-milwaukee-ruby-users-group-december-2010</id>
    <content type="html"><![CDATA[<p>While I was going to write a whole blog post (essentially what I said during the talk) I think these slides stand on their own pretty well.</p>
<div style="">
<object height="355" width="425">
<param name="movie" value="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=rails3slides2-101230222626-phpapp01&amp;stripped_title=whats-new-and-great-in-rails-3-matt-gauger-milwaukee-ruby-users-group-december-2010&amp;userName=mathiasx" />
<param name="allowFullScreen" value="true" />
<param name="allowScriptAccess" value="always" /><embed src="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=rails3slides2-101230222626-phpapp01&amp;stripped_title=whats-new-and-great-in-rails-3-matt-gauger-milwaukee-ruby-users-group-december-2010&amp;userName=mathiasx" type="application/x-shockwave-flash" height="355" width="425"></embed>
</object>
<div style="padding: 5px 0 12px;">View more <a href="http://www.slideshare.net/">presentations</a> from <a href="http://www.slideshare.net/mathiasx">mathiasx</a>.</div>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Techniques for learning a new skill]]></title>
    <link href="http://blog.mattgauger.com/blog/2010/12/26/techniques-for-learning-a-new-skill/"/>
    <updated>2010-12-26T00:00:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2010/12/26/techniques-for-learning-a-new-skill</id>
    <content type="html"><![CDATA[<p>I just found these notes in an old notebook and figured they deserved a blog post . This isn&#8217;t self-help type stuff, it&#8217;s just practical. I&#8217;m blogging this so that some day I find it when reading my old blog posts, just as I found the list in an old notebook.</p>
<p>Techniques helpful in learning any skill:</p>
<ul>
<li>Know your starting point (measure it, somehow) =&gt; &#8220;You can&#8217;t manage what you can&#8217;t measure.&#8221;</li>
<li>Set objectives.</li>
<li>Have techniques for improvement available.</li>
<li>Assess your progress after a reasonable period.</li>
<li>Have strategies for continuing improvement and maintaining the skill.</li>
</ul>
<p>I&#8217;d add in there to have a good way to record your progress. Something like <a href="http://www.daytum.com/">Daytum</a> will probably work if you want a web-based tool. Excel spreadsheets and notebooks work great, too.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presentation: LAMP vs. The World]]></title>
    <link href="http://blog.mattgauger.com/blog/2010/12/14/presentation-lamp-vs-the-world/"/>
    <updated>2010-12-14T00:00:00-06:00</updated>
    <id>http://blog.mattgauger.com/blog/2010/12/14/presentation-lamp-vs-the-world</id>
    <content type="html"><![CDATA[<strong style="display: block; margin: 12px 0 4px;"><a href="http://www.slideshare.net/mathiasx/matt-gauger-lamp-vs-the-world-mke-php-users-group-december-14-2010" title="Matt Gauger - Lamp vs. the world - MKE PHP Users Group - December 14, 2010 ">Matt Gauger - Lamp vs. the world - MKE PHP Users Group - December 14, 2010 </a></strong> 
<object height="355" width="425">
<param name="movie" value="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=lampvs-theworld-101214192052-phpapp01&amp;stripped_title=matt-gauger-lamp-vs-the-world-mke-php-users-group-december-14-2010&amp;userName=mathiasx" />
<param name="allowFullScreen" value="true" />
<param name="allowScriptAccess" value="always" /><embed src="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=lampvs-theworld-101214192052-phpapp01&amp;stripped_title=matt-gauger-lamp-vs-the-world-mke-php-users-group-december-14-2010&amp;userName=mathiasx" type="application/x-shockwave-flash" height="355" width="425"></embed>
</object>
<div style="padding: 5px 0 12px;">View more <a href="http://www.slideshare.net/">presentations</a> from <a href="http://www.slideshare.net/mathiasx">mathiasx</a>.</div>


]]></content>
  </entry>
  
</feed>
